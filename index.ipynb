{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of                 Product                       Consumer complaint narrative\n",
      "0          Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
      "1          Student loan  I am being contacted by a debt collector for p...\n",
      "2          Student loan  I cosigned XXXX student loans at SallieMae for...\n",
      "3          Student loan  Navient has sytematically and illegally failed...\n",
      "4          Student loan  My wife became eligible for XXXX Loan Forgiven...\n",
      "5          Student loan  Hello, I am a XXXX resident who has multiple X...\n",
      "6          Student loan  My account was sold to them and expect me to p...\n",
      "7          Student loan  On XX/XX/XXXX, I was sued in XXXX County, XXXX...\n",
      "8          Student loan  To Whom It May Concern, I applied for a privat...\n",
      "9          Student loan  My loans through XXXX were deferred as I am a ...\n",
      "10         Student loan  National Collegiate and trust has been on my C...\n",
      "11         Student loan  Just received information about the lawsuit fi...\n",
      "12         Student loan  Calls are being made to all different numbers ...\n",
      "13         Student loan  My loans are in the process of being transferr...\n",
      "14         Student loan  I have a $ XXXX+ loan with AES and have tried ...\n",
      "15         Student loan  I was told that the Fed Loan Help company was ...\n",
      "16         Student loan  My private loans are serviced through AES. I l...\n",
      "17         Student loan  I have had SEVERAL issues with XXXX XXXX/Navie...\n",
      "18         Student loan  XX/XX/XXXX, I graduated from XXXX. Unknown to ...\n",
      "19         Student loan  I have already filed several complaints about ...\n",
      "20         Student loan  I feel as if my student loans through Navient ...\n",
      "21         Student loan  I have XXXX variable interest rate loans throu...\n",
      "22         Student loan  I have student loans totaling slightly over {$...\n",
      "23         Student loan  I tried to consolidate my loan into my home lo...\n",
      "24         Student loan  Was constantly told I had to pay a high monthl...\n",
      "25         Student loan  I refinanced my loan from Navient XX/XX/XXXX. ...\n",
      "26         Student loan  ACS has been applying my entire monthly paymen...\n",
      "27         Student loan  Enclosed is all the information that is attatc...\n",
      "28         Student loan  Navient is committing the same offenses they c...\n",
      "29         Student loan  I had a student loan from XX/XX/XXXX.for {$250...\n",
      "...                 ...                                                ...\n",
      "59970  Credit reporting  When asked to verify my identity with various ...\n",
      "59971  Credit reporting  I have XXXX accounts on my credit report XXXX ...\n",
      "59972  Credit reporting  XXXX is continuously re-aging the accounts on ...\n",
      "59973  Credit reporting  on both of my credit reports XXXX XXXX and Equ...\n",
      "59974  Credit reporting  While checking my personal credit report, I no...\n",
      "59975  Credit reporting  XXXX is listing wrong information on my credit...\n",
      "59976  Credit reporting  equifax IS REPORTING, a bankruptcy on my repor...\n",
      "59977  Credit reporting  Equifax has changed my report but does n't all...\n",
      "59978  Credit reporting  For the past several months, I have been dispu...\n",
      "59979  Credit reporting  I have student loans that do n't belong to me ...\n",
      "59980  Credit reporting  This dispute has been faxed twice already and ...\n",
      "59981  Credit reporting  In 2010, a rental car company in XXXX charged ...\n",
      "59982  Credit reporting  A few months ago I had a problem with XXXX XXX...\n",
      "59983  Credit reporting  This is getting ridiculous. TransUnion sent a ...\n",
      "59984  Credit reporting  Experian XXXX, XXXX collection was removed aft...\n",
      "59985  Credit reporting  I contacted the Bankruptcy Court asking their ...\n",
      "59986  Credit reporting  I have a lot of Hard inquieres, on my credit r...\n",
      "59987  Credit reporting  XXXX XXXX XXXX account charged off in XXXX, in...\n",
      "59988  Credit reporting  Notice of federal tax Lien is on my public rec...\n",
      "59989  Credit reporting  This item was disputed with TransUnion. I rece...\n",
      "59990  Credit reporting  There are XXXX liens that are inaccurately rep...\n",
      "59991  Credit reporting  I can not get a credit report from experian wi...\n",
      "59992  Credit reporting  I have dispute an item with Equifax. That item...\n",
      "59993  Credit reporting  I am unable to view my credit report or score ...\n",
      "59994  Credit reporting  On XXXX XXXX, 2015 I informed Equifax that the...\n",
      "59995  Credit reporting  In XXXX, I defaulted on a credit card with XXX...\n",
      "59996  Credit reporting  I HAVE CALLED TRANSUNION JUST ABOUT EVERY MONT...\n",
      "59997  Credit reporting  Hello, there is a 2012 Chapter XXXX Bankruptcy...\n",
      "59998  Credit reporting  I had a 2 yr internet contract with XXXX XXXX ...\n",
      "59999  Credit reporting  I filed a complaint against XXXX XXXX XXXX wit...\n",
      "\n",
      "[60000 rows x 2 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "print (df.info)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>While I have not been personally affected by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I have a student loan with American Education ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am currently going through a bankruptcy and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>This is a follow up to dispute : XXXX Bank of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>I defaulted on a loan with OMNI Financial arou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Product                       Consumer complaint narrative\n",
       "0   Student loan  While I have not been personally affected by t...\n",
       "1   Student loan  I have a student loan with American Education ...\n",
       "2   Student loan  I am currently going through a bankruptcy and ...\n",
       "3    Credit card  This is a follow up to dispute : XXXX Bank of ...\n",
       "4  Consumer Loan  I defaulted on a loan with OMNI Financial arou..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index = range(10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = df['Product']\n",
    "complaint = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(complaint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = tokenizer.texts_to_matrix(complaint, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_encoder, product_onehot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.9622 - acc: 0.1452 - val_loss: 1.9469 - val_acc: 0.1680\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9303 - acc: 0.1940 - val_loss: 1.9250 - val_acc: 0.1920\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9062 - acc: 0.2219 - val_loss: 1.9047 - val_acc: 0.2160\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8826 - acc: 0.2441 - val_loss: 1.8829 - val_acc: 0.2280\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8572 - acc: 0.2655 - val_loss: 1.8581 - val_acc: 0.2490\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8281 - acc: 0.2885 - val_loss: 1.8276 - val_acc: 0.2700\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7930 - acc: 0.3156 - val_loss: 1.7886 - val_acc: 0.3090\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7510 - acc: 0.3465 - val_loss: 1.7435 - val_acc: 0.3390\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7024 - acc: 0.3804 - val_loss: 1.6917 - val_acc: 0.3890\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6483 - acc: 0.4177 - val_loss: 1.6361 - val_acc: 0.4320\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5915 - acc: 0.4573 - val_loss: 1.5786 - val_acc: 0.4720\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5332 - acc: 0.4924 - val_loss: 1.5193 - val_acc: 0.4990\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4740 - acc: 0.5213 - val_loss: 1.4595 - val_acc: 0.5320\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4155 - acc: 0.5513 - val_loss: 1.4021 - val_acc: 0.5640\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3583 - acc: 0.5825 - val_loss: 1.3434 - val_acc: 0.5810\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3034 - acc: 0.6025 - val_loss: 1.2918 - val_acc: 0.6010\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2509 - acc: 0.6199 - val_loss: 1.2396 - val_acc: 0.6050\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2019 - acc: 0.6377 - val_loss: 1.1914 - val_acc: 0.6300\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1552 - acc: 0.6499 - val_loss: 1.1466 - val_acc: 0.6470\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1115 - acc: 0.6651 - val_loss: 1.1041 - val_acc: 0.6570\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0714 - acc: 0.6741 - val_loss: 1.0663 - val_acc: 0.6700\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0344 - acc: 0.6845 - val_loss: 1.0282 - val_acc: 0.6810\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0000 - acc: 0.6905 - val_loss: 0.9967 - val_acc: 0.6910\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9684 - acc: 0.6999 - val_loss: 0.9681 - val_acc: 0.6950\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9388 - acc: 0.7079 - val_loss: 0.9422 - val_acc: 0.7000\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9118 - acc: 0.7117 - val_loss: 0.9157 - val_acc: 0.6960\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8865 - acc: 0.7184 - val_loss: 0.8940 - val_acc: 0.7120\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8628 - acc: 0.7259 - val_loss: 0.8719 - val_acc: 0.7160\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8416 - acc: 0.7299 - val_loss: 0.8552 - val_acc: 0.7180\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8212 - acc: 0.7320 - val_loss: 0.8331 - val_acc: 0.7240\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8026 - acc: 0.7372 - val_loss: 0.8176 - val_acc: 0.7270\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7850 - acc: 0.7431 - val_loss: 0.8031 - val_acc: 0.7270\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7683 - acc: 0.7460 - val_loss: 0.7890 - val_acc: 0.7360\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7530 - acc: 0.7505 - val_loss: 0.7754 - val_acc: 0.7320\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7387 - acc: 0.7545 - val_loss: 0.7647 - val_acc: 0.7360\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7248 - acc: 0.7561 - val_loss: 0.7552 - val_acc: 0.7480\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7119 - acc: 0.7601 - val_loss: 0.7434 - val_acc: 0.7440\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6997 - acc: 0.7636 - val_loss: 0.7339 - val_acc: 0.7420\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6879 - acc: 0.7657 - val_loss: 0.7290 - val_acc: 0.7530\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6770 - acc: 0.7692 - val_loss: 0.7184 - val_acc: 0.7570\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6667 - acc: 0.7725 - val_loss: 0.7130 - val_acc: 0.7540\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6567 - acc: 0.7759 - val_loss: 0.7000 - val_acc: 0.7540\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6471 - acc: 0.7791 - val_loss: 0.6932 - val_acc: 0.7590\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6380 - acc: 0.7836 - val_loss: 0.6903 - val_acc: 0.7620\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6297 - acc: 0.7877 - val_loss: 0.6849 - val_acc: 0.7600\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6212 - acc: 0.7887 - val_loss: 0.6754 - val_acc: 0.7630\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6126 - acc: 0.7921 - val_loss: 0.6735 - val_acc: 0.7670\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6055 - acc: 0.7945 - val_loss: 0.6653 - val_acc: 0.7630\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5980 - acc: 0.7983 - val_loss: 0.6600 - val_acc: 0.7690\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5906 - acc: 0.8019 - val_loss: 0.6587 - val_acc: 0.7670\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5833 - acc: 0.8052 - val_loss: 0.6576 - val_acc: 0.7620\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5770 - acc: 0.8051 - val_loss: 0.6507 - val_acc: 0.7690\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5704 - acc: 0.8072 - val_loss: 0.6455 - val_acc: 0.7730\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5639 - acc: 0.8101 - val_loss: 0.6427 - val_acc: 0.7710\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5580 - acc: 0.8131 - val_loss: 0.6388 - val_acc: 0.7740\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5517 - acc: 0.8155 - val_loss: 0.6341 - val_acc: 0.7720\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5461 - acc: 0.8181 - val_loss: 0.6328 - val_acc: 0.7720\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5405 - acc: 0.8209 - val_loss: 0.6298 - val_acc: 0.7790\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5351 - acc: 0.8216 - val_loss: 0.6329 - val_acc: 0.7760\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5299 - acc: 0.8221 - val_loss: 0.6234 - val_acc: 0.7770\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5247 - acc: 0.8265 - val_loss: 0.6258 - val_acc: 0.7740\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5192 - acc: 0.8267 - val_loss: 0.6191 - val_acc: 0.7790\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5140 - acc: 0.8299 - val_loss: 0.6206 - val_acc: 0.7830\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5093 - acc: 0.8359 - val_loss: 0.6151 - val_acc: 0.7830\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5044 - acc: 0.8352 - val_loss: 0.6122 - val_acc: 0.7830\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.4999 - acc: 0.8377 - val_loss: 0.6133 - val_acc: 0.7800\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4953 - acc: 0.8389 - val_loss: 0.6099 - val_acc: 0.7820\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4904 - acc: 0.8403 - val_loss: 0.6092 - val_acc: 0.7850\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4856 - acc: 0.8416 - val_loss: 0.6051 - val_acc: 0.7840\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4815 - acc: 0.8433 - val_loss: 0.6059 - val_acc: 0.7890\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4773 - acc: 0.8447 - val_loss: 0.6029 - val_acc: 0.7850\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4731 - acc: 0.8453 - val_loss: 0.6090 - val_acc: 0.7800\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4693 - acc: 0.8472 - val_loss: 0.5971 - val_acc: 0.7880\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4645 - acc: 0.8504 - val_loss: 0.6022 - val_acc: 0.7840\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4610 - acc: 0.8525 - val_loss: 0.5987 - val_acc: 0.7800\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4572 - acc: 0.8529 - val_loss: 0.5968 - val_acc: 0.7900\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4534 - acc: 0.8520 - val_loss: 0.5977 - val_acc: 0.7830\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4491 - acc: 0.8548 - val_loss: 0.5966 - val_acc: 0.7910\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4456 - acc: 0.8559 - val_loss: 0.5956 - val_acc: 0.7840\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4421 - acc: 0.8565 - val_loss: 0.5947 - val_acc: 0.7840\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4390 - acc: 0.8593 - val_loss: 0.5943 - val_acc: 0.7840\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4356 - acc: 0.8592 - val_loss: 0.5927 - val_acc: 0.7900\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4312 - acc: 0.8604 - val_loss: 0.5882 - val_acc: 0.7880\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4278 - acc: 0.8613 - val_loss: 0.5883 - val_acc: 0.7890\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4239 - acc: 0.8653 - val_loss: 0.5883 - val_acc: 0.7880\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4208 - acc: 0.8655 - val_loss: 0.5874 - val_acc: 0.7900\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4168 - acc: 0.8688 - val_loss: 0.5872 - val_acc: 0.7910\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.4140 - acc: 0.8675 - val_loss: 0.5858 - val_acc: 0.7880\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4108 - acc: 0.8668 - val_loss: 0.5844 - val_acc: 0.7930\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4073 - acc: 0.8692 - val_loss: 0.5849 - val_acc: 0.7900\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4041 - acc: 0.8687 - val_loss: 0.5816 - val_acc: 0.7900\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4012 - acc: 0.8709 - val_loss: 0.5893 - val_acc: 0.7810\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3986 - acc: 0.8739 - val_loss: 0.5837 - val_acc: 0.7880\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3948 - acc: 0.8732 - val_loss: 0.5857 - val_acc: 0.7880\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3921 - acc: 0.8731 - val_loss: 0.5828 - val_acc: 0.7880\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3888 - acc: 0.8759 - val_loss: 0.5920 - val_acc: 0.7810\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3864 - acc: 0.8753 - val_loss: 0.5814 - val_acc: 0.7900\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3830 - acc: 0.8757 - val_loss: 0.5819 - val_acc: 0.7860\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3800 - acc: 0.8787 - val_loss: 0.5817 - val_acc: 0.7930\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3775 - acc: 0.8784 - val_loss: 0.5803 - val_acc: 0.7870\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3743 - acc: 0.8776 - val_loss: 0.5788 - val_acc: 0.7890\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3722 - acc: 0.8803 - val_loss: 0.5888 - val_acc: 0.7850\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3693 - acc: 0.8817 - val_loss: 0.5798 - val_acc: 0.7820\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3664 - acc: 0.8840 - val_loss: 0.5812 - val_acc: 0.7920\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3636 - acc: 0.8848 - val_loss: 0.5812 - val_acc: 0.7920\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3610 - acc: 0.8832 - val_loss: 0.5813 - val_acc: 0.7840\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3584 - acc: 0.8868 - val_loss: 0.5825 - val_acc: 0.7830\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3557 - acc: 0.8869 - val_loss: 0.5775 - val_acc: 0.7960\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.3530 - acc: 0.8884 - val_loss: 0.5801 - val_acc: 0.7840\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3503 - acc: 0.8896 - val_loss: 0.5782 - val_acc: 0.7870\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3483 - acc: 0.8900 - val_loss: 0.5825 - val_acc: 0.7940\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3454 - acc: 0.8919 - val_loss: 0.5825 - val_acc: 0.7920\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3431 - acc: 0.8927 - val_loss: 0.5824 - val_acc: 0.7840\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3405 - acc: 0.8935 - val_loss: 0.5823 - val_acc: 0.7860\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3383 - acc: 0.8960 - val_loss: 0.5839 - val_acc: 0.7860\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3355 - acc: 0.8945 - val_loss: 0.5832 - val_acc: 0.7900\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3333 - acc: 0.8971 - val_loss: 0.5829 - val_acc: 0.7830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3310 - acc: 0.8976 - val_loss: 0.5808 - val_acc: 0.7960\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3285 - acc: 0.8967 - val_loss: 0.5815 - val_acc: 0.7890\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3263 - acc: 0.8995 - val_loss: 0.5847 - val_acc: 0.7880\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 40us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3229493920644124, 0.9025333333015442]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6181016111373902, 0.7673333333333333]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9469 - acc: 0.1749 - val_loss: 1.9352 - val_acc: 0.1840\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9274 - acc: 0.2033 - val_loss: 1.9192 - val_acc: 0.2030\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9083 - acc: 0.2220 - val_loss: 1.9028 - val_acc: 0.2210\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8875 - acc: 0.2372 - val_loss: 1.8841 - val_acc: 0.2360\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8655 - acc: 0.2564 - val_loss: 1.8637 - val_acc: 0.2550\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8415 - acc: 0.2759 - val_loss: 1.8406 - val_acc: 0.2870\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8149 - acc: 0.3080 - val_loss: 1.8133 - val_acc: 0.3190\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7842 - acc: 0.3459 - val_loss: 1.7812 - val_acc: 0.3600\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7489 - acc: 0.3803 - val_loss: 1.7445 - val_acc: 0.3850\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7089 - acc: 0.4112 - val_loss: 1.7031 - val_acc: 0.4200\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6649 - acc: 0.4431 - val_loss: 1.6578 - val_acc: 0.4330\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6172 - acc: 0.4752 - val_loss: 1.6088 - val_acc: 0.4690\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5665 - acc: 0.5012 - val_loss: 1.5572 - val_acc: 0.5000\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5144 - acc: 0.5236 - val_loss: 1.5037 - val_acc: 0.5280\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4612 - acc: 0.5420 - val_loss: 1.4514 - val_acc: 0.5380\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4088 - acc: 0.5568 - val_loss: 1.4015 - val_acc: 0.5480\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3579 - acc: 0.5767 - val_loss: 1.3527 - val_acc: 0.5680\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3089 - acc: 0.5956 - val_loss: 1.3027 - val_acc: 0.5860\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2616 - acc: 0.6093 - val_loss: 1.2575 - val_acc: 0.6010\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2171 - acc: 0.6217 - val_loss: 1.2150 - val_acc: 0.6110\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1753 - acc: 0.6325 - val_loss: 1.1760 - val_acc: 0.6230\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1360 - acc: 0.6452 - val_loss: 1.1381 - val_acc: 0.6340\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0996 - acc: 0.6540 - val_loss: 1.1038 - val_acc: 0.6390\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0653 - acc: 0.6613 - val_loss: 1.0706 - val_acc: 0.6480\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0338 - acc: 0.6705 - val_loss: 1.0407 - val_acc: 0.6580\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0042 - acc: 0.6805 - val_loss: 1.0154 - val_acc: 0.6640\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9767 - acc: 0.6833 - val_loss: 0.9872 - val_acc: 0.6700\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9512 - acc: 0.6901 - val_loss: 0.9625 - val_acc: 0.6770\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9274 - acc: 0.7007 - val_loss: 0.9415 - val_acc: 0.6860\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9052 - acc: 0.7047 - val_loss: 0.9213 - val_acc: 0.6910\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8845 - acc: 0.7085 - val_loss: 0.9031 - val_acc: 0.6990\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8648 - acc: 0.7133 - val_loss: 0.8872 - val_acc: 0.7040\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8470 - acc: 0.7191 - val_loss: 0.8715 - val_acc: 0.7040\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8301 - acc: 0.7207 - val_loss: 0.8548 - val_acc: 0.7130\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8139 - acc: 0.7252 - val_loss: 0.8430 - val_acc: 0.7090\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7988 - acc: 0.7319 - val_loss: 0.8263 - val_acc: 0.7200\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7848 - acc: 0.7349 - val_loss: 0.8171 - val_acc: 0.7200\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7710 - acc: 0.7395 - val_loss: 0.8046 - val_acc: 0.7260\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7587 - acc: 0.7417 - val_loss: 0.7960 - val_acc: 0.7210\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7465 - acc: 0.7451 - val_loss: 0.7846 - val_acc: 0.7270\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7352 - acc: 0.7483 - val_loss: 0.7797 - val_acc: 0.7240\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7241 - acc: 0.7527 - val_loss: 0.7664 - val_acc: 0.7360\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7133 - acc: 0.7551 - val_loss: 0.7586 - val_acc: 0.7310\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7039 - acc: 0.7587 - val_loss: 0.7511 - val_acc: 0.7390\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6938 - acc: 0.7609 - val_loss: 0.7436 - val_acc: 0.7370\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6851 - acc: 0.7632 - val_loss: 0.7383 - val_acc: 0.7340\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6759 - acc: 0.7675 - val_loss: 0.7327 - val_acc: 0.7360\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6676 - acc: 0.7697 - val_loss: 0.7269 - val_acc: 0.7380\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6597 - acc: 0.7701 - val_loss: 0.7204 - val_acc: 0.7430\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6512 - acc: 0.7732 - val_loss: 0.7174 - val_acc: 0.7340\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6439 - acc: 0.7772 - val_loss: 0.7125 - val_acc: 0.7370\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6362 - acc: 0.7785 - val_loss: 0.7031 - val_acc: 0.7430\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6295 - acc: 0.7797 - val_loss: 0.6967 - val_acc: 0.7490\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6224 - acc: 0.7836 - val_loss: 0.6934 - val_acc: 0.7560\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6156 - acc: 0.7844 - val_loss: 0.6898 - val_acc: 0.7520\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6092 - acc: 0.7873 - val_loss: 0.6856 - val_acc: 0.7580\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6026 - acc: 0.7884 - val_loss: 0.6808 - val_acc: 0.7550\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5967 - acc: 0.7913 - val_loss: 0.6785 - val_acc: 0.7540\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5904 - acc: 0.7937 - val_loss: 0.6776 - val_acc: 0.7550\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5846 - acc: 0.7956 - val_loss: 0.6731 - val_acc: 0.7580\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 24us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5810887812296549, 0.7964000000317891]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69605970398585, 0.7406666669845581]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 2.6133 - acc: 0.1477 - val_loss: 2.5956 - val_acc: 0.1610\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.5881 - acc: 0.1829 - val_loss: 2.5768 - val_acc: 0.1890\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.5636 - acc: 0.2220 - val_loss: 2.5541 - val_acc: 0.2280\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.5349 - acc: 0.2463 - val_loss: 2.5301 - val_acc: 0.2310\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.5060 - acc: 0.2636 - val_loss: 2.5048 - val_acc: 0.2450\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.4765 - acc: 0.2861 - val_loss: 2.4777 - val_acc: 0.2700\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.4444 - acc: 0.3073 - val_loss: 2.4464 - val_acc: 0.2860\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.4081 - acc: 0.3296 - val_loss: 2.4106 - val_acc: 0.2990\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.3677 - acc: 0.3505 - val_loss: 2.3691 - val_acc: 0.3210\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.3238 - acc: 0.3715 - val_loss: 2.3257 - val_acc: 0.3450\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.2773 - acc: 0.3900 - val_loss: 2.2801 - val_acc: 0.3550\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.2291 - acc: 0.4107 - val_loss: 2.2309 - val_acc: 0.3800\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.1790 - acc: 0.4325 - val_loss: 2.1798 - val_acc: 0.4040\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.1282 - acc: 0.4547 - val_loss: 2.1288 - val_acc: 0.4340\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.0775 - acc: 0.4852 - val_loss: 2.0803 - val_acc: 0.4650\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.0276 - acc: 0.5103 - val_loss: 2.0302 - val_acc: 0.4880\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9786 - acc: 0.5359 - val_loss: 1.9793 - val_acc: 0.5220\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9312 - acc: 0.5551 - val_loss: 1.9326 - val_acc: 0.5510\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8861 - acc: 0.5832 - val_loss: 1.8874 - val_acc: 0.5730\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8424 - acc: 0.6021 - val_loss: 1.8453 - val_acc: 0.5860\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8021 - acc: 0.6132 - val_loss: 1.8050 - val_acc: 0.6050\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7632 - acc: 0.6263 - val_loss: 1.7691 - val_acc: 0.6180\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7268 - acc: 0.6379 - val_loss: 1.7336 - val_acc: 0.6220\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6924 - acc: 0.6472 - val_loss: 1.7015 - val_acc: 0.6370\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6598 - acc: 0.6559 - val_loss: 1.6693 - val_acc: 0.6420\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6296 - acc: 0.6623 - val_loss: 1.6394 - val_acc: 0.6460\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6017 - acc: 0.6725 - val_loss: 1.6121 - val_acc: 0.6520\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5749 - acc: 0.6800 - val_loss: 1.5880 - val_acc: 0.6650\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5497 - acc: 0.6864 - val_loss: 1.5631 - val_acc: 0.6630\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5266 - acc: 0.6907 - val_loss: 1.5428 - val_acc: 0.6700\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5050 - acc: 0.6961 - val_loss: 1.5231 - val_acc: 0.6770\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4841 - acc: 0.7011 - val_loss: 1.5019 - val_acc: 0.6780\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4650 - acc: 0.7036 - val_loss: 1.4845 - val_acc: 0.6850\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4466 - acc: 0.7101 - val_loss: 1.4675 - val_acc: 0.6830\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4294 - acc: 0.7124 - val_loss: 1.4538 - val_acc: 0.6850\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4131 - acc: 0.7179 - val_loss: 1.4374 - val_acc: 0.6930\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3975 - acc: 0.7207 - val_loss: 1.4250 - val_acc: 0.6950\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3826 - acc: 0.7239 - val_loss: 1.4123 - val_acc: 0.7070\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3683 - acc: 0.7292 - val_loss: 1.3986 - val_acc: 0.7070\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3552 - acc: 0.7325 - val_loss: 1.3890 - val_acc: 0.7090\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3423 - acc: 0.7361 - val_loss: 1.3772 - val_acc: 0.7090\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3301 - acc: 0.7392 - val_loss: 1.3670 - val_acc: 0.7190\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3182 - acc: 0.7408 - val_loss: 1.3554 - val_acc: 0.7290\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3068 - acc: 0.7472 - val_loss: 1.3491 - val_acc: 0.7200\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2960 - acc: 0.7447 - val_loss: 1.3386 - val_acc: 0.7250\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2856 - acc: 0.7481 - val_loss: 1.3297 - val_acc: 0.7340\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2754 - acc: 0.7521 - val_loss: 1.3202 - val_acc: 0.7250\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2654 - acc: 0.7525 - val_loss: 1.3150 - val_acc: 0.7270\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2554 - acc: 0.7555 - val_loss: 1.3034 - val_acc: 0.7330\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2462 - acc: 0.7603 - val_loss: 1.2973 - val_acc: 0.7340\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2368 - acc: 0.7597 - val_loss: 1.2908 - val_acc: 0.7410\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2285 - acc: 0.7644 - val_loss: 1.2841 - val_acc: 0.7350\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2195 - acc: 0.7651 - val_loss: 1.2777 - val_acc: 0.7360\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2111 - acc: 0.7647 - val_loss: 1.2699 - val_acc: 0.7430\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2028 - acc: 0.7665 - val_loss: 1.2671 - val_acc: 0.7420\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1950 - acc: 0.7687 - val_loss: 1.2566 - val_acc: 0.7400\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1875 - acc: 0.7716 - val_loss: 1.2519 - val_acc: 0.7450\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1791 - acc: 0.7744 - val_loss: 1.2466 - val_acc: 0.7430\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1716 - acc: 0.7767 - val_loss: 1.2413 - val_acc: 0.7420\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1643 - acc: 0.7785 - val_loss: 1.2360 - val_acc: 0.7410\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1577 - acc: 0.7792 - val_loss: 1.2303 - val_acc: 0.7380\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1502 - acc: 0.7804 - val_loss: 1.2220 - val_acc: 0.7410\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1433 - acc: 0.7843 - val_loss: 1.2194 - val_acc: 0.7410\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1364 - acc: 0.7873 - val_loss: 1.2184 - val_acc: 0.7480\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1299 - acc: 0.7868 - val_loss: 1.2100 - val_acc: 0.7460\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1233 - acc: 0.7888 - val_loss: 1.2041 - val_acc: 0.7510\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1170 - acc: 0.7900 - val_loss: 1.1993 - val_acc: 0.7480\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1106 - acc: 0.7936 - val_loss: 1.1976 - val_acc: 0.7500\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1044 - acc: 0.7957 - val_loss: 1.1914 - val_acc: 0.7480\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0982 - acc: 0.7972 - val_loss: 1.1863 - val_acc: 0.7510\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0920 - acc: 0.7992 - val_loss: 1.1874 - val_acc: 0.7500\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0866 - acc: 0.7999 - val_loss: 1.1792 - val_acc: 0.7520\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0800 - acc: 0.8017 - val_loss: 1.1747 - val_acc: 0.7510\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0743 - acc: 0.8047 - val_loss: 1.1757 - val_acc: 0.7490\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0687 - acc: 0.8047 - val_loss: 1.1660 - val_acc: 0.7540\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0636 - acc: 0.8055 - val_loss: 1.1643 - val_acc: 0.7550\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0576 - acc: 0.8063 - val_loss: 1.1565 - val_acc: 0.7590\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0524 - acc: 0.8091 - val_loss: 1.1541 - val_acc: 0.7540\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0466 - acc: 0.8097 - val_loss: 1.1506 - val_acc: 0.7620\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0413 - acc: 0.8109 - val_loss: 1.1470 - val_acc: 0.7600\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0361 - acc: 0.8128 - val_loss: 1.1455 - val_acc: 0.7620\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0310 - acc: 0.8131 - val_loss: 1.1469 - val_acc: 0.7580\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0260 - acc: 0.8139 - val_loss: 1.1417 - val_acc: 0.7600\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0205 - acc: 0.8160 - val_loss: 1.1355 - val_acc: 0.7610\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0156 - acc: 0.8187 - val_loss: 1.1320 - val_acc: 0.7610\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0107 - acc: 0.8192 - val_loss: 1.1272 - val_acc: 0.7650\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0055 - acc: 0.8212 - val_loss: 1.1277 - val_acc: 0.7610\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0012 - acc: 0.8220 - val_loss: 1.1230 - val_acc: 0.7620\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9962 - acc: 0.8223 - val_loss: 1.1220 - val_acc: 0.7640\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9916 - acc: 0.8240 - val_loss: 1.1152 - val_acc: 0.7690\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9869 - acc: 0.8249 - val_loss: 1.1136 - val_acc: 0.7660\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9823 - acc: 0.8259 - val_loss: 1.1078 - val_acc: 0.7690\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9777 - acc: 0.8259 - val_loss: 1.1096 - val_acc: 0.7700\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9733 - acc: 0.8259 - val_loss: 1.1067 - val_acc: 0.7680\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9690 - acc: 0.8288 - val_loss: 1.1006 - val_acc: 0.7680\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9642 - acc: 0.8324 - val_loss: 1.1003 - val_acc: 0.7710\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9602 - acc: 0.8304 - val_loss: 1.0939 - val_acc: 0.7660\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9557 - acc: 0.8349 - val_loss: 1.0953 - val_acc: 0.7730\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9516 - acc: 0.8336 - val_loss: 1.0900 - val_acc: 0.7770\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9475 - acc: 0.8365 - val_loss: 1.0883 - val_acc: 0.7740\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9432 - acc: 0.8367 - val_loss: 1.0886 - val_acc: 0.7710\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9391 - acc: 0.8371 - val_loss: 1.0867 - val_acc: 0.7690\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9352 - acc: 0.8369 - val_loss: 1.0792 - val_acc: 0.7770\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9310 - acc: 0.8407 - val_loss: 1.0798 - val_acc: 0.7760\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9269 - acc: 0.8384 - val_loss: 1.0740 - val_acc: 0.7760\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9235 - acc: 0.8417 - val_loss: 1.0730 - val_acc: 0.7780\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9192 - acc: 0.8421 - val_loss: 1.0697 - val_acc: 0.7790\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9153 - acc: 0.8443 - val_loss: 1.0677 - val_acc: 0.7810\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9117 - acc: 0.8441 - val_loss: 1.0703 - val_acc: 0.7770\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9078 - acc: 0.8445 - val_loss: 1.0698 - val_acc: 0.7700\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9039 - acc: 0.8451 - val_loss: 1.0678 - val_acc: 0.7730\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9003 - acc: 0.8471 - val_loss: 1.0634 - val_acc: 0.7700\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8967 - acc: 0.8468 - val_loss: 1.0589 - val_acc: 0.7740\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8927 - acc: 0.8464 - val_loss: 1.0577 - val_acc: 0.7820\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8891 - acc: 0.8496 - val_loss: 1.0542 - val_acc: 0.7800\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8855 - acc: 0.8497 - val_loss: 1.0523 - val_acc: 0.7760\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8818 - acc: 0.8499 - val_loss: 1.0500 - val_acc: 0.7690\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8784 - acc: 0.8525 - val_loss: 1.0497 - val_acc: 0.7780\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8749 - acc: 0.8539 - val_loss: 1.0459 - val_acc: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8716 - acc: 0.8524 - val_loss: 1.0442 - val_acc: 0.7770\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 15.9947 - acc: 0.1468 - val_loss: 15.5882 - val_acc: 0.1580\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 15.2370 - acc: 0.1623 - val_loss: 14.8477 - val_acc: 0.1710\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 14.5068 - acc: 0.1720 - val_loss: 14.1315 - val_acc: 0.1840\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 13.7997 - acc: 0.1871 - val_loss: 13.4370 - val_acc: 0.1910\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 13.1137 - acc: 0.2067 - val_loss: 12.7629 - val_acc: 0.2000\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 12.4487 - acc: 0.2245 - val_loss: 12.1096 - val_acc: 0.2250\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 11.8045 - acc: 0.2428 - val_loss: 11.4772 - val_acc: 0.2430\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 11.1805 - acc: 0.2700 - val_loss: 10.8642 - val_acc: 0.2660\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 10.5763 - acc: 0.2967 - val_loss: 10.2700 - val_acc: 0.2860\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 9.9915 - acc: 0.3235 - val_loss: 9.6955 - val_acc: 0.3240\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 9.4257 - acc: 0.3613 - val_loss: 9.1400 - val_acc: 0.3520\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 8.8791 - acc: 0.3924 - val_loss: 8.6036 - val_acc: 0.3920\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 8.3525 - acc: 0.4279 - val_loss: 8.0880 - val_acc: 0.4140\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 7.8466 - acc: 0.4608 - val_loss: 7.5942 - val_acc: 0.4490\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 7.3620 - acc: 0.4932 - val_loss: 7.1209 - val_acc: 0.4840\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 6.8993 - acc: 0.5161 - val_loss: 6.6704 - val_acc: 0.5250\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 6.4587 - acc: 0.5451 - val_loss: 6.2420 - val_acc: 0.5380\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 6.0402 - acc: 0.5656 - val_loss: 5.8357 - val_acc: 0.5720\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 5.6441 - acc: 0.5823 - val_loss: 5.4515 - val_acc: 0.5860\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 5.2694 - acc: 0.5963 - val_loss: 5.0891 - val_acc: 0.5940\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 4.9162 - acc: 0.6020 - val_loss: 4.7482 - val_acc: 0.6100\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 4.5849 - acc: 0.6141 - val_loss: 4.4295 - val_acc: 0.6060\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 4.2755 - acc: 0.6165 - val_loss: 4.1326 - val_acc: 0.6110\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.9882 - acc: 0.6228 - val_loss: 3.8570 - val_acc: 0.6210\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.7225 - acc: 0.6273 - val_loss: 3.6039 - val_acc: 0.6130\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 3.4782 - acc: 0.6244 - val_loss: 3.3711 - val_acc: 0.6120\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 3.2553 - acc: 0.6272 - val_loss: 3.1598 - val_acc: 0.6090\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.0530 - acc: 0.6235 - val_loss: 2.9690 - val_acc: 0.6100\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.8717 - acc: 0.6285 - val_loss: 2.7969 - val_acc: 0.6130\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.7115 - acc: 0.6292 - val_loss: 2.6465 - val_acc: 0.6180\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.5722 - acc: 0.6308 - val_loss: 2.5184 - val_acc: 0.6230\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.4529 - acc: 0.6321 - val_loss: 2.4094 - val_acc: 0.6120\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.3534 - acc: 0.6287 - val_loss: 2.3207 - val_acc: 0.6240\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.2730 - acc: 0.6359 - val_loss: 2.2477 - val_acc: 0.6270\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.2106 - acc: 0.6387 - val_loss: 2.1919 - val_acc: 0.6190\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.1629 - acc: 0.6384 - val_loss: 2.1513 - val_acc: 0.6180\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.1270 - acc: 0.6409 - val_loss: 2.1198 - val_acc: 0.6200\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.0976 - acc: 0.6435 - val_loss: 2.0901 - val_acc: 0.6240\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.0711 - acc: 0.6459 - val_loss: 2.0645 - val_acc: 0.6300\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.0472 - acc: 0.6471 - val_loss: 2.0432 - val_acc: 0.6440\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.0249 - acc: 0.6523 - val_loss: 2.0183 - val_acc: 0.6430\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.0036 - acc: 0.6541 - val_loss: 1.9977 - val_acc: 0.6500\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9842 - acc: 0.6553 - val_loss: 1.9768 - val_acc: 0.6580\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9650 - acc: 0.6565 - val_loss: 1.9587 - val_acc: 0.6610\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9469 - acc: 0.6603 - val_loss: 1.9416 - val_acc: 0.6570\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9300 - acc: 0.6608 - val_loss: 1.9241 - val_acc: 0.6610\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9139 - acc: 0.6647 - val_loss: 1.9072 - val_acc: 0.6620\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8979 - acc: 0.6665 - val_loss: 1.8895 - val_acc: 0.6640\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8825 - acc: 0.6696 - val_loss: 1.8764 - val_acc: 0.6610\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8686 - acc: 0.6696 - val_loss: 1.8585 - val_acc: 0.6700\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8541 - acc: 0.6733 - val_loss: 1.8466 - val_acc: 0.6710\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8409 - acc: 0.6740 - val_loss: 1.8346 - val_acc: 0.6690\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8276 - acc: 0.6745 - val_loss: 1.8186 - val_acc: 0.6730\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8151 - acc: 0.6767 - val_loss: 1.8054 - val_acc: 0.6730\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8028 - acc: 0.6780 - val_loss: 1.7951 - val_acc: 0.6780\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7908 - acc: 0.6801 - val_loss: 1.7816 - val_acc: 0.6780\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7794 - acc: 0.6816 - val_loss: 1.7730 - val_acc: 0.6790\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7682 - acc: 0.6833 - val_loss: 1.7559 - val_acc: 0.6840\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7568 - acc: 0.6849 - val_loss: 1.7461 - val_acc: 0.6810\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7460 - acc: 0.6855 - val_loss: 1.7354 - val_acc: 0.6870\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7358 - acc: 0.6863 - val_loss: 1.7247 - val_acc: 0.6890\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7258 - acc: 0.6867 - val_loss: 1.7143 - val_acc: 0.6930\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7155 - acc: 0.6869 - val_loss: 1.7039 - val_acc: 0.6880\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7057 - acc: 0.6879 - val_loss: 1.6949 - val_acc: 0.6880\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6963 - acc: 0.6880 - val_loss: 1.6858 - val_acc: 0.6900\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6872 - acc: 0.6892 - val_loss: 1.6817 - val_acc: 0.6830\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6783 - acc: 0.6908 - val_loss: 1.6658 - val_acc: 0.6960\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6688 - acc: 0.6912 - val_loss: 1.6570 - val_acc: 0.6950\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6599 - acc: 0.6917 - val_loss: 1.6472 - val_acc: 0.6940\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6513 - acc: 0.6928 - val_loss: 1.6389 - val_acc: 0.6950\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6429 - acc: 0.6924 - val_loss: 1.6315 - val_acc: 0.6940\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6345 - acc: 0.6937 - val_loss: 1.6225 - val_acc: 0.7000\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6263 - acc: 0.6944 - val_loss: 1.6151 - val_acc: 0.7000\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6183 - acc: 0.6948 - val_loss: 1.6045 - val_acc: 0.6990\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6102 - acc: 0.6949 - val_loss: 1.6013 - val_acc: 0.6970\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6023 - acc: 0.6956 - val_loss: 1.5945 - val_acc: 0.7020\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5948 - acc: 0.6965 - val_loss: 1.5862 - val_acc: 0.6980\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5869 - acc: 0.6969 - val_loss: 1.5739 - val_acc: 0.7030\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5789 - acc: 0.6989 - val_loss: 1.5676 - val_acc: 0.7040\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5713 - acc: 0.6972 - val_loss: 1.5593 - val_acc: 0.7020\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5647 - acc: 0.6981 - val_loss: 1.5523 - val_acc: 0.7050\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5572 - acc: 0.6983 - val_loss: 1.5429 - val_acc: 0.7030\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5495 - acc: 0.6989 - val_loss: 1.5379 - val_acc: 0.7080\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5422 - acc: 0.6980 - val_loss: 1.5297 - val_acc: 0.7030\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5353 - acc: 0.6988 - val_loss: 1.5268 - val_acc: 0.7110\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5285 - acc: 0.6991 - val_loss: 1.5153 - val_acc: 0.7070\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5215 - acc: 0.7001 - val_loss: 1.5085 - val_acc: 0.7060\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5148 - acc: 0.6989 - val_loss: 1.5035 - val_acc: 0.7130\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.5081 - acc: 0.7011 - val_loss: 1.4965 - val_acc: 0.7110\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.5010 - acc: 0.7013 - val_loss: 1.4903 - val_acc: 0.7100\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.4946 - acc: 0.7028 - val_loss: 1.4812 - val_acc: 0.7120\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.4881 - acc: 0.7028 - val_loss: 1.4746 - val_acc: 0.7120\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4816 - acc: 0.7040 - val_loss: 1.4687 - val_acc: 0.7050\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.4748 - acc: 0.7045 - val_loss: 1.4627 - val_acc: 0.7170\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.4686 - acc: 0.7027 - val_loss: 1.4543 - val_acc: 0.7120\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4622 - acc: 0.7041 - val_loss: 1.4492 - val_acc: 0.7080\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.4561 - acc: 0.7040 - val_loss: 1.4408 - val_acc: 0.7150\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.4499 - acc: 0.7045 - val_loss: 1.4362 - val_acc: 0.7090\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.4436 - acc: 0.7064 - val_loss: 1.4293 - val_acc: 0.7100\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4379 - acc: 0.7060 - val_loss: 1.4236 - val_acc: 0.7160\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4319 - acc: 0.7064 - val_loss: 1.4179 - val_acc: 0.7210\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4255 - acc: 0.7079 - val_loss: 1.4118 - val_acc: 0.7160\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4197 - acc: 0.7077 - val_loss: 1.4076 - val_acc: 0.7220\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4137 - acc: 0.7069 - val_loss: 1.4021 - val_acc: 0.7120\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4077 - acc: 0.7079 - val_loss: 1.3942 - val_acc: 0.7220\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4020 - acc: 0.7079 - val_loss: 1.3875 - val_acc: 0.7200\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3965 - acc: 0.7076 - val_loss: 1.3841 - val_acc: 0.7250\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3911 - acc: 0.7091 - val_loss: 1.3779 - val_acc: 0.7240\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3855 - acc: 0.7096 - val_loss: 1.3732 - val_acc: 0.7170\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3802 - acc: 0.7097 - val_loss: 1.3664 - val_acc: 0.7210\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3740 - acc: 0.7104 - val_loss: 1.3598 - val_acc: 0.7230\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3690 - acc: 0.7119 - val_loss: 1.3569 - val_acc: 0.7220\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3636 - acc: 0.7116 - val_loss: 1.3508 - val_acc: 0.7250\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3581 - acc: 0.7133 - val_loss: 1.3501 - val_acc: 0.7240\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3534 - acc: 0.7117 - val_loss: 1.3416 - val_acc: 0.7220\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3479 - acc: 0.7135 - val_loss: 1.3376 - val_acc: 0.7230\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3428 - acc: 0.7132 - val_loss: 1.3302 - val_acc: 0.7250\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3380 - acc: 0.7149 - val_loss: 1.3244 - val_acc: 0.7290\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3333 - acc: 0.7144 - val_loss: 1.3197 - val_acc: 0.7280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3277 - acc: 0.7145 - val_loss: 1.3153 - val_acc: 0.7310\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 16.0343 - acc: 0.1116 - val_loss: 15.6165 - val_acc: 0.1340\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 15.2620 - acc: 0.1555 - val_loss: 14.8695 - val_acc: 0.1730\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 14.5262 - acc: 0.2044 - val_loss: 14.1507 - val_acc: 0.2000\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 13.8168 - acc: 0.2241 - val_loss: 13.4557 - val_acc: 0.2080\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 13.1299 - acc: 0.2305 - val_loss: 12.7824 - val_acc: 0.2140\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 12.4642 - acc: 0.2364 - val_loss: 12.1291 - val_acc: 0.2150\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 11.8182 - acc: 0.2461 - val_loss: 11.4948 - val_acc: 0.2350\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 11.1909 - acc: 0.2657 - val_loss: 10.8783 - val_acc: 0.2570\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 10.6808 - acc: 0.29 - 0s 24us/step - loss: 10.5807 - acc: 0.2943 - val_loss: 10.2775 - val_acc: 0.3020\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 9.9878 - acc: 0.3319 - val_loss: 9.6943 - val_acc: 0.3210\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 9.4131 - acc: 0.3559 - val_loss: 9.1295 - val_acc: 0.3470\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 8.8570 - acc: 0.3873 - val_loss: 8.5850 - val_acc: 0.3920\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 8.3203 - acc: 0.4179 - val_loss: 8.0586 - val_acc: 0.4120\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 7.8036 - acc: 0.4389 - val_loss: 7.5545 - val_acc: 0.4400\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 7.3084 - acc: 0.4667 - val_loss: 7.0709 - val_acc: 0.4710\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 6.8356 - acc: 0.4969 - val_loss: 6.6110 - val_acc: 0.4940\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 6.3863 - acc: 0.5239 - val_loss: 6.1745 - val_acc: 0.5050\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 5.9603 - acc: 0.5464 - val_loss: 5.7608 - val_acc: 0.5370\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 5.5574 - acc: 0.5687 - val_loss: 5.3720 - val_acc: 0.5730\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 5.1766 - acc: 0.6012 - val_loss: 5.0008 - val_acc: 0.5670\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 4.8184 - acc: 0.6080 - val_loss: 4.6537 - val_acc: 0.5700\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.4838 - acc: 0.6211 - val_loss: 4.3335 - val_acc: 0.5970\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 4.1730 - acc: 0.6348 - val_loss: 4.0328 - val_acc: 0.6190\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 3.8855 - acc: 0.6452 - val_loss: 3.7599 - val_acc: 0.6400\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 3.6206 - acc: 0.6531 - val_loss: 3.5069 - val_acc: 0.6450\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.3785 - acc: 0.6587 - val_loss: 3.2746 - val_acc: 0.6380\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 3.1584 - acc: 0.6615 - val_loss: 3.0659 - val_acc: 0.6390\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.9602 - acc: 0.6665 - val_loss: 2.8784 - val_acc: 0.6390\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.7835 - acc: 0.6639 - val_loss: 2.7127 - val_acc: 0.6620\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.6280 - acc: 0.6696 - val_loss: 2.5690 - val_acc: 0.6640\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.4933 - acc: 0.6692 - val_loss: 2.4441 - val_acc: 0.6720\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.3785 - acc: 0.6728 - val_loss: 2.3393 - val_acc: 0.6590\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.2829 - acc: 0.6715 - val_loss: 2.2498 - val_acc: 0.6670\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.2063 - acc: 0.6735 - val_loss: 2.1840 - val_acc: 0.6640\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.1473 - acc: 0.6756 - val_loss: 2.1317 - val_acc: 0.6730\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.1030 - acc: 0.6760 - val_loss: 2.0984 - val_acc: 0.6810\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.0705 - acc: 0.6741 - val_loss: 2.0694 - val_acc: 0.6600\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.0435 - acc: 0.6741 - val_loss: 2.0427 - val_acc: 0.6580\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.0202 - acc: 0.6736 - val_loss: 2.0184 - val_acc: 0.6810\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9986 - acc: 0.6791 - val_loss: 1.9948 - val_acc: 0.6760\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9786 - acc: 0.6788 - val_loss: 1.9748 - val_acc: 0.6780\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9601 - acc: 0.6772 - val_loss: 1.9561 - val_acc: 0.6760\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9419 - acc: 0.6784 - val_loss: 1.9380 - val_acc: 0.6810\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9250 - acc: 0.6804 - val_loss: 1.9215 - val_acc: 0.6750\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9087 - acc: 0.6797 - val_loss: 1.9037 - val_acc: 0.6750\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8927 - acc: 0.6789 - val_loss: 1.8885 - val_acc: 0.6750\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8780 - acc: 0.6808 - val_loss: 1.8740 - val_acc: 0.6870\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8631 - acc: 0.6831 - val_loss: 1.8584 - val_acc: 0.6750\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8488 - acc: 0.6817 - val_loss: 1.8460 - val_acc: 0.6710\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8348 - acc: 0.6823 - val_loss: 1.8309 - val_acc: 0.6850\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8218 - acc: 0.6847 - val_loss: 1.8173 - val_acc: 0.6850\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8088 - acc: 0.6852 - val_loss: 1.8021 - val_acc: 0.6900\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7963 - acc: 0.6861 - val_loss: 1.7918 - val_acc: 0.6850\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7839 - acc: 0.6867 - val_loss: 1.7797 - val_acc: 0.6700\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7724 - acc: 0.6871 - val_loss: 1.7670 - val_acc: 0.6840\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7604 - acc: 0.6865 - val_loss: 1.7559 - val_acc: 0.6920\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7486 - acc: 0.6889 - val_loss: 1.7462 - val_acc: 0.6880\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7379 - acc: 0.6885 - val_loss: 1.7320 - val_acc: 0.6890\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7267 - acc: 0.6909 - val_loss: 1.7210 - val_acc: 0.6900\n",
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7165 - acc: 0.6904 - val_loss: 1.7111 - val_acc: 0.6860\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7061 - acc: 0.6893 - val_loss: 1.7002 - val_acc: 0.6850\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6963 - acc: 0.6891 - val_loss: 1.6898 - val_acc: 0.6890\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6862 - acc: 0.6917 - val_loss: 1.6808 - val_acc: 0.6930\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6766 - acc: 0.6920 - val_loss: 1.6755 - val_acc: 0.6940\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6672 - acc: 0.6913 - val_loss: 1.6633 - val_acc: 0.7020\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6580 - acc: 0.6933 - val_loss: 1.6509 - val_acc: 0.6930\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.6493 - acc: 0.6943 - val_loss: 1.6442 - val_acc: 0.6950\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6402 - acc: 0.6943 - val_loss: 1.6346 - val_acc: 0.6900\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6314 - acc: 0.6947 - val_loss: 1.6238 - val_acc: 0.6960\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6231 - acc: 0.6941 - val_loss: 1.6167 - val_acc: 0.6900\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.6143 - acc: 0.6952 - val_loss: 1.6077 - val_acc: 0.6990\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.6062 - acc: 0.6951 - val_loss: 1.5988 - val_acc: 0.7040\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.5985 - acc: 0.6967 - val_loss: 1.5906 - val_acc: 0.6890\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.5903 - acc: 0.6967 - val_loss: 1.5856 - val_acc: 0.6970\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5829 - acc: 0.6967 - val_loss: 1.5737 - val_acc: 0.6970\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5749 - acc: 0.6973 - val_loss: 1.5685 - val_acc: 0.7050\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5677 - acc: 0.6969 - val_loss: 1.5638 - val_acc: 0.7020\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5601 - acc: 0.6977 - val_loss: 1.5549 - val_acc: 0.7090\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.5525 - acc: 0.6997 - val_loss: 1.5501 - val_acc: 0.6930\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5459 - acc: 0.6993 - val_loss: 1.5395 - val_acc: 0.6950\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5384 - acc: 0.7004 - val_loss: 1.5388 - val_acc: 0.7050\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5320 - acc: 0.7004 - val_loss: 1.5270 - val_acc: 0.6980\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.5249 - acc: 0.7024 - val_loss: 1.5195 - val_acc: 0.6850\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5180 - acc: 0.7020 - val_loss: 1.5097 - val_acc: 0.7010\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5110 - acc: 0.7023 - val_loss: 1.5041 - val_acc: 0.6920\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.5046 - acc: 0.7017 - val_loss: 1.4998 - val_acc: 0.7060\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4981 - acc: 0.7031 - val_loss: 1.4971 - val_acc: 0.6920\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4917 - acc: 0.7031 - val_loss: 1.4835 - val_acc: 0.7040\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4853 - acc: 0.7015 - val_loss: 1.4823 - val_acc: 0.7140\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4787 - acc: 0.7048 - val_loss: 1.4750 - val_acc: 0.7110\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4724 - acc: 0.7057 - val_loss: 1.4665 - val_acc: 0.7080\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4663 - acc: 0.7047 - val_loss: 1.4613 - val_acc: 0.7080\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4603 - acc: 0.7033 - val_loss: 1.4536 - val_acc: 0.7120\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4539 - acc: 0.7060 - val_loss: 1.4464 - val_acc: 0.7110\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4479 - acc: 0.7057 - val_loss: 1.4445 - val_acc: 0.7090\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4423 - acc: 0.7060 - val_loss: 1.4352 - val_acc: 0.7130\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4362 - acc: 0.7061 - val_loss: 1.4296 - val_acc: 0.7070\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4299 - acc: 0.7085 - val_loss: 1.4270 - val_acc: 0.7120\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4242 - acc: 0.7087 - val_loss: 1.4180 - val_acc: 0.7040\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4182 - acc: 0.7084 - val_loss: 1.4105 - val_acc: 0.7080\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4127 - acc: 0.7081 - val_loss: 1.4081 - val_acc: 0.7100\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4073 - acc: 0.7097 - val_loss: 1.4046 - val_acc: 0.7150\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4017 - acc: 0.7073 - val_loss: 1.3975 - val_acc: 0.7090\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3964 - acc: 0.7109 - val_loss: 1.3927 - val_acc: 0.7040\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3909 - acc: 0.7099 - val_loss: 1.3869 - val_acc: 0.7140\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3852 - acc: 0.7105 - val_loss: 1.3817 - val_acc: 0.7100\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3802 - acc: 0.7105 - val_loss: 1.3742 - val_acc: 0.7070\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3747 - acc: 0.7116 - val_loss: 1.3716 - val_acc: 0.7090\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3697 - acc: 0.7132 - val_loss: 1.3719 - val_acc: 0.7120\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3647 - acc: 0.7128 - val_loss: 1.3671 - val_acc: 0.7090\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3593 - acc: 0.7133 - val_loss: 1.3539 - val_acc: 0.7080\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3543 - acc: 0.7124 - val_loss: 1.3508 - val_acc: 0.7050\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3495 - acc: 0.7133 - val_loss: 1.3467 - val_acc: 0.7160\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3447 - acc: 0.7141 - val_loss: 1.3396 - val_acc: 0.7100\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3398 - acc: 0.7131 - val_loss: 1.3392 - val_acc: 0.7190\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3344 - acc: 0.7167 - val_loss: 1.3329 - val_acc: 0.7180\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3303 - acc: 0.7153 - val_loss: 1.3257 - val_acc: 0.7170\n",
      "Epoch 118/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3258 - acc: 0.7152 - val_loss: 1.3236 - val_acc: 0.7110\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3208 - acc: 0.7163 - val_loss: 1.3226 - val_acc: 0.7090\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3161 - acc: 0.7155 - val_loss: 1.3159 - val_acc: 0.7090\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3115 - acc: 0.7165 - val_loss: 1.3056 - val_acc: 0.7140\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3069 - acc: 0.7183 - val_loss: 1.3092 - val_acc: 0.7000\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3032 - acc: 0.7180 - val_loss: 1.3003 - val_acc: 0.7150\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2981 - acc: 0.7179 - val_loss: 1.2938 - val_acc: 0.7250\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2938 - acc: 0.7179 - val_loss: 1.2953 - val_acc: 0.7240\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2892 - acc: 0.7181 - val_loss: 1.2839 - val_acc: 0.7210\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2853 - acc: 0.7192 - val_loss: 1.2811 - val_acc: 0.7250\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2802 - acc: 0.7195 - val_loss: 1.2748 - val_acc: 0.7160\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2765 - acc: 0.7196 - val_loss: 1.2748 - val_acc: 0.7230\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2721 - acc: 0.7189 - val_loss: 1.2699 - val_acc: 0.7240\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.2680 - acc: 0.7203 - val_loss: 1.2637 - val_acc: 0.7160\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2636 - acc: 0.7205 - val_loss: 1.2607 - val_acc: 0.7190\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2604 - acc: 0.7200 - val_loss: 1.2603 - val_acc: 0.7200\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2565 - acc: 0.7193 - val_loss: 1.2581 - val_acc: 0.7240\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2529 - acc: 0.7201 - val_loss: 1.2482 - val_acc: 0.7230\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2487 - acc: 0.7227 - val_loss: 1.2521 - val_acc: 0.7180\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2449 - acc: 0.7217 - val_loss: 1.2432 - val_acc: 0.7180\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2408 - acc: 0.7223 - val_loss: 1.2387 - val_acc: 0.7170\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2369 - acc: 0.7227 - val_loss: 1.2382 - val_acc: 0.7250\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2336 - acc: 0.7235 - val_loss: 1.2418 - val_acc: 0.7250\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2300 - acc: 0.7216 - val_loss: 1.2275 - val_acc: 0.7230\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2265 - acc: 0.7231 - val_loss: 1.2254 - val_acc: 0.7260\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2233 - acc: 0.7237 - val_loss: 1.2278 - val_acc: 0.7320\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2201 - acc: 0.7219 - val_loss: 1.2166 - val_acc: 0.7260\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2164 - acc: 0.7233 - val_loss: 1.2166 - val_acc: 0.7220\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2130 - acc: 0.7225 - val_loss: 1.2216 - val_acc: 0.7160\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2102 - acc: 0.7227 - val_loss: 1.2124 - val_acc: 0.7220\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2066 - acc: 0.7240 - val_loss: 1.2032 - val_acc: 0.7230\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2026 - acc: 0.7236 - val_loss: 1.2014 - val_acc: 0.7220\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1998 - acc: 0.7239 - val_loss: 1.2044 - val_acc: 0.7300\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1966 - acc: 0.7241 - val_loss: 1.1968 - val_acc: 0.7240\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1934 - acc: 0.7271 - val_loss: 1.1952 - val_acc: 0.7220\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1902 - acc: 0.7251 - val_loss: 1.1885 - val_acc: 0.7240\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1875 - acc: 0.7255 - val_loss: 1.1863 - val_acc: 0.7230\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1838 - acc: 0.7259 - val_loss: 1.1857 - val_acc: 0.7260\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1809 - acc: 0.7259 - val_loss: 1.1822 - val_acc: 0.7180\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1781 - acc: 0.7269 - val_loss: 1.1833 - val_acc: 0.7160\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1760 - acc: 0.7252 - val_loss: 1.1814 - val_acc: 0.7310\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1725 - acc: 0.7257 - val_loss: 1.1727 - val_acc: 0.7240\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1693 - acc: 0.7285 - val_loss: 1.1702 - val_acc: 0.7240\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1664 - acc: 0.7275 - val_loss: 1.1733 - val_acc: 0.7210\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1639 - acc: 0.7279 - val_loss: 1.1754 - val_acc: 0.7260\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1614 - acc: 0.7285 - val_loss: 1.1624 - val_acc: 0.7290\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1587 - acc: 0.7273 - val_loss: 1.1603 - val_acc: 0.7250\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1561 - acc: 0.7307 - val_loss: 1.1611 - val_acc: 0.7330\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1528 - acc: 0.7281 - val_loss: 1.1581 - val_acc: 0.7330\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1500 - acc: 0.7299 - val_loss: 1.1513 - val_acc: 0.7250\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1471 - acc: 0.7293 - val_loss: 1.1489 - val_acc: 0.7250\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1447 - acc: 0.7285 - val_loss: 1.1487 - val_acc: 0.7330\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1420 - acc: 0.7296 - val_loss: 1.1444 - val_acc: 0.7220\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1402 - acc: 0.7281 - val_loss: 1.1419 - val_acc: 0.7250\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1379 - acc: 0.7289 - val_loss: 1.1436 - val_acc: 0.7290\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1356 - acc: 0.7300 - val_loss: 1.1370 - val_acc: 0.7240\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1328 - acc: 0.7319 - val_loss: 1.1359 - val_acc: 0.7300\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1308 - acc: 0.7304 - val_loss: 1.1331 - val_acc: 0.7270\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1290 - acc: 0.7309 - val_loss: 1.1345 - val_acc: 0.7290\n",
      "Epoch 177/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1261 - acc: 0.7297 - val_loss: 1.1268 - val_acc: 0.7310\n",
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1246 - acc: 0.7320 - val_loss: 1.1279 - val_acc: 0.7280\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1220 - acc: 0.7308 - val_loss: 1.1277 - val_acc: 0.7280\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1206 - acc: 0.7296 - val_loss: 1.1262 - val_acc: 0.7310\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1179 - acc: 0.7329 - val_loss: 1.1252 - val_acc: 0.7260\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1164 - acc: 0.7319 - val_loss: 1.1252 - val_acc: 0.7290\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1143 - acc: 0.7320 - val_loss: 1.1201 - val_acc: 0.7280\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1129 - acc: 0.7327 - val_loss: 1.1201 - val_acc: 0.7290\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1102 - acc: 0.7340 - val_loss: 1.1148 - val_acc: 0.7310\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1079 - acc: 0.7325 - val_loss: 1.1147 - val_acc: 0.7250\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1064 - acc: 0.7328 - val_loss: 1.1107 - val_acc: 0.7310\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1042 - acc: 0.7340 - val_loss: 1.1125 - val_acc: 0.7290\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1028 - acc: 0.7345 - val_loss: 1.1099 - val_acc: 0.7280\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1012 - acc: 0.7351 - val_loss: 1.1036 - val_acc: 0.7260\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0994 - acc: 0.7336 - val_loss: 1.1031 - val_acc: 0.7270\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0975 - acc: 0.7357 - val_loss: 1.1052 - val_acc: 0.7250\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0961 - acc: 0.7332 - val_loss: 1.1026 - val_acc: 0.7300\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0946 - acc: 0.7351 - val_loss: 1.1016 - val_acc: 0.7350\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0926 - acc: 0.7353 - val_loss: 1.0961 - val_acc: 0.7360\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0903 - acc: 0.7352 - val_loss: 1.0990 - val_acc: 0.7300\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0892 - acc: 0.7361 - val_loss: 1.0942 - val_acc: 0.7300\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0872 - acc: 0.7368 - val_loss: 1.0934 - val_acc: 0.7350\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0861 - acc: 0.7379 - val_loss: 1.0952 - val_acc: 0.7360\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0841 - acc: 0.7371 - val_loss: 1.0918 - val_acc: 0.7270\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0824 - acc: 0.7365 - val_loss: 1.0905 - val_acc: 0.7330\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0812 - acc: 0.7369 - val_loss: 1.0951 - val_acc: 0.7310\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0795 - acc: 0.7375 - val_loss: 1.0842 - val_acc: 0.7270\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.0773 - acc: 0.736 - 0s 27us/step - loss: 1.0782 - acc: 0.7367 - val_loss: 1.0884 - val_acc: 0.7310\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0770 - acc: 0.7376 - val_loss: 1.0850 - val_acc: 0.7360\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0757 - acc: 0.7388 - val_loss: 1.0837 - val_acc: 0.7260\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0741 - acc: 0.7369 - val_loss: 1.0826 - val_acc: 0.7330\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0729 - acc: 0.7371 - val_loss: 1.0817 - val_acc: 0.7290\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0710 - acc: 0.7372 - val_loss: 1.0794 - val_acc: 0.7270\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0699 - acc: 0.7381 - val_loss: 1.0761 - val_acc: 0.7310\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0685 - acc: 0.7381 - val_loss: 1.0756 - val_acc: 0.7300\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0664 - acc: 0.7391 - val_loss: 1.0752 - val_acc: 0.7330\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0655 - acc: 0.7389 - val_loss: 1.0740 - val_acc: 0.7340\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0641 - acc: 0.7408 - val_loss: 1.0708 - val_acc: 0.7350\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0631 - acc: 0.7389 - val_loss: 1.0697 - val_acc: 0.7270\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0616 - acc: 0.7393 - val_loss: 1.0771 - val_acc: 0.7390\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0596 - acc: 0.7401 - val_loss: 1.0702 - val_acc: 0.7320\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0586 - acc: 0.7412 - val_loss: 1.0683 - val_acc: 0.7330\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0574 - acc: 0.7403 - val_loss: 1.0680 - val_acc: 0.7360\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0556 - acc: 0.7408 - val_loss: 1.0662 - val_acc: 0.7360\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0549 - acc: 0.7415 - val_loss: 1.0639 - val_acc: 0.7330\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0535 - acc: 0.7400 - val_loss: 1.0631 - val_acc: 0.7350\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0519 - acc: 0.7421 - val_loss: 1.0655 - val_acc: 0.7360\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0505 - acc: 0.7396 - val_loss: 1.0664 - val_acc: 0.7320\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0501 - acc: 0.7425 - val_loss: 1.0612 - val_acc: 0.7350\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0483 - acc: 0.7429 - val_loss: 1.0557 - val_acc: 0.7350\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0467 - acc: 0.7409 - val_loss: 1.0550 - val_acc: 0.7370\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0457 - acc: 0.7428 - val_loss: 1.0637 - val_acc: 0.7250\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0450 - acc: 0.7436 - val_loss: 1.0586 - val_acc: 0.7360\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0428 - acc: 0.7427 - val_loss: 1.0520 - val_acc: 0.7320\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0428 - acc: 0.7440 - val_loss: 1.0548 - val_acc: 0.7380\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0408 - acc: 0.7425 - val_loss: 1.0511 - val_acc: 0.7380\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0401 - acc: 0.7449 - val_loss: 1.0600 - val_acc: 0.7390\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0382 - acc: 0.7433 - val_loss: 1.0484 - val_acc: 0.7340\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0373 - acc: 0.7436 - val_loss: 1.0481 - val_acc: 0.7370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0360 - acc: 0.7451 - val_loss: 1.0454 - val_acc: 0.7390\n",
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0354 - acc: 0.7439 - val_loss: 1.0429 - val_acc: 0.7370\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0337 - acc: 0.7451 - val_loss: 1.0566 - val_acc: 0.7360\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0339 - acc: 0.7437 - val_loss: 1.0421 - val_acc: 0.7370\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0326 - acc: 0.7459 - val_loss: 1.0491 - val_acc: 0.7400\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0310 - acc: 0.7461 - val_loss: 1.0404 - val_acc: 0.7340\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0301 - acc: 0.7465 - val_loss: 1.0444 - val_acc: 0.7410\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0284 - acc: 0.7453 - val_loss: 1.0408 - val_acc: 0.7380\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0279 - acc: 0.7453 - val_loss: 1.0447 - val_acc: 0.7400\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0277 - acc: 0.7460 - val_loss: 1.0410 - val_acc: 0.7330\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0262 - acc: 0.7460 - val_loss: 1.0352 - val_acc: 0.7400\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0238 - acc: 0.7472 - val_loss: 1.0391 - val_acc: 0.7350\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0247 - acc: 0.7455 - val_loss: 1.0357 - val_acc: 0.7410\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0229 - acc: 0.7481 - val_loss: 1.0328 - val_acc: 0.7420\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0214 - acc: 0.7471 - val_loss: 1.0388 - val_acc: 0.7290\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0202 - acc: 0.7480 - val_loss: 1.0380 - val_acc: 0.7440\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0190 - acc: 0.7479 - val_loss: 1.0515 - val_acc: 0.7370\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0190 - acc: 0.7469 - val_loss: 1.0357 - val_acc: 0.7410\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0179 - acc: 0.7492 - val_loss: 1.0300 - val_acc: 0.7330\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0170 - acc: 0.7476 - val_loss: 1.0269 - val_acc: 0.7450\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0146 - acc: 0.7489 - val_loss: 1.0282 - val_acc: 0.7430\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0148 - acc: 0.7487 - val_loss: 1.0304 - val_acc: 0.7350\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0135 - acc: 0.7481 - val_loss: 1.0316 - val_acc: 0.7380\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0126 - acc: 0.7503 - val_loss: 1.0280 - val_acc: 0.7410\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0114 - acc: 0.7479 - val_loss: 1.0247 - val_acc: 0.7440\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0100 - acc: 0.7484 - val_loss: 1.0282 - val_acc: 0.7390\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0094 - acc: 0.7492 - val_loss: 1.0243 - val_acc: 0.7420\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0084 - acc: 0.7493 - val_loss: 1.0351 - val_acc: 0.7360\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0085 - acc: 0.7483 - val_loss: 1.0235 - val_acc: 0.7360\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0067 - acc: 0.7507 - val_loss: 1.0214 - val_acc: 0.7440\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0059 - acc: 0.7521 - val_loss: 1.0240 - val_acc: 0.7420\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0046 - acc: 0.7511 - val_loss: 1.0198 - val_acc: 0.7400\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0041 - acc: 0.7520 - val_loss: 1.0190 - val_acc: 0.7380\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0027 - acc: 0.7507 - val_loss: 1.0221 - val_acc: 0.7390\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0024 - acc: 0.7512 - val_loss: 1.0259 - val_acc: 0.7350\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0023 - acc: 0.7507 - val_loss: 1.0183 - val_acc: 0.7400\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0003 - acc: 0.7511 - val_loss: 1.0147 - val_acc: 0.7470\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9992 - acc: 0.7532 - val_loss: 1.0189 - val_acc: 0.7400\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9987 - acc: 0.7529 - val_loss: 1.0143 - val_acc: 0.7470\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9979 - acc: 0.7505 - val_loss: 1.0240 - val_acc: 0.7410\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9972 - acc: 0.7505 - val_loss: 1.0154 - val_acc: 0.7360\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9959 - acc: 0.7527 - val_loss: 1.0153 - val_acc: 0.7420\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9948 - acc: 0.7519 - val_loss: 1.0092 - val_acc: 0.7460\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9947 - acc: 0.7528 - val_loss: 1.0178 - val_acc: 0.7410\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9936 - acc: 0.7516 - val_loss: 1.0221 - val_acc: 0.7380\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9931 - acc: 0.7527 - val_loss: 1.0073 - val_acc: 0.7450\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9923 - acc: 0.7544 - val_loss: 1.0066 - val_acc: 0.7420\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9915 - acc: 0.7536 - val_loss: 1.0073 - val_acc: 0.7400\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9898 - acc: 0.7545 - val_loss: 1.0106 - val_acc: 0.7440\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9887 - acc: 0.7552 - val_loss: 1.0141 - val_acc: 0.7420\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9885 - acc: 0.7527 - val_loss: 1.0165 - val_acc: 0.7440\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9879 - acc: 0.7547 - val_loss: 1.0088 - val_acc: 0.7410\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9865 - acc: 0.7555 - val_loss: 1.0061 - val_acc: 0.7390\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9865 - acc: 0.7548 - val_loss: 1.0053 - val_acc: 0.7400\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9861 - acc: 0.7549 - val_loss: 1.0055 - val_acc: 0.7410\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9841 - acc: 0.7557 - val_loss: 1.0056 - val_acc: 0.7430\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9837 - acc: 0.7549 - val_loss: 1.0134 - val_acc: 0.7400\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9836 - acc: 0.7556 - val_loss: 1.0026 - val_acc: 0.7430\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9821 - acc: 0.7553 - val_loss: 1.0010 - val_acc: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9819 - acc: 0.7553 - val_loss: 1.0070 - val_acc: 0.7500\n",
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9820 - acc: 0.7556 - val_loss: 1.0010 - val_acc: 0.7410\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9809 - acc: 0.7553 - val_loss: 0.9994 - val_acc: 0.7430\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9795 - acc: 0.7547 - val_loss: 0.9962 - val_acc: 0.7430\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9782 - acc: 0.7568 - val_loss: 1.0047 - val_acc: 0.7410\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9784 - acc: 0.7559 - val_loss: 0.9991 - val_acc: 0.7420\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9773 - acc: 0.7571 - val_loss: 0.9952 - val_acc: 0.7420\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9761 - acc: 0.7561 - val_loss: 0.9997 - val_acc: 0.7430\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9762 - acc: 0.7565 - val_loss: 0.9984 - val_acc: 0.7420\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9756 - acc: 0.7581 - val_loss: 0.9981 - val_acc: 0.7440\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9745 - acc: 0.7591 - val_loss: 1.0003 - val_acc: 0.7410\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9736 - acc: 0.7584 - val_loss: 0.9959 - val_acc: 0.7470\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9727 - acc: 0.7581 - val_loss: 0.9947 - val_acc: 0.7410\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9732 - acc: 0.7572 - val_loss: 0.9956 - val_acc: 0.7420\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9717 - acc: 0.7571 - val_loss: 0.9951 - val_acc: 0.7450\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9710 - acc: 0.7597 - val_loss: 0.9906 - val_acc: 0.7430\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9703 - acc: 0.7605 - val_loss: 0.9915 - val_acc: 0.7430\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9695 - acc: 0.7580 - val_loss: 0.9906 - val_acc: 0.7440\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9695 - acc: 0.7600 - val_loss: 1.0077 - val_acc: 0.7410\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9689 - acc: 0.7595 - val_loss: 0.9931 - val_acc: 0.7440\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9671 - acc: 0.7580 - val_loss: 0.9928 - val_acc: 0.7470\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9679 - acc: 0.7580 - val_loss: 0.9911 - val_acc: 0.7450\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9662 - acc: 0.7593 - val_loss: 0.9871 - val_acc: 0.7430\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9653 - acc: 0.7615 - val_loss: 0.9871 - val_acc: 0.7440\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9658 - acc: 0.7596 - val_loss: 0.9910 - val_acc: 0.7440\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9638 - acc: 0.7601 - val_loss: 0.9979 - val_acc: 0.7500\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9635 - acc: 0.7588 - val_loss: 1.0057 - val_acc: 0.7480\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9632 - acc: 0.7603 - val_loss: 0.9838 - val_acc: 0.7440\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9623 - acc: 0.7600 - val_loss: 0.9875 - val_acc: 0.7430\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9624 - acc: 0.7599 - val_loss: 0.9839 - val_acc: 0.7450\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9611 - acc: 0.7607 - val_loss: 0.9907 - val_acc: 0.7480\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9603 - acc: 0.7624 - val_loss: 0.9821 - val_acc: 0.7440\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9594 - acc: 0.7603 - val_loss: 0.9846 - val_acc: 0.7430\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9596 - acc: 0.7604 - val_loss: 0.9831 - val_acc: 0.7450\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9585 - acc: 0.7589 - val_loss: 0.9841 - val_acc: 0.7450\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9582 - acc: 0.7613 - val_loss: 0.9817 - val_acc: 0.7430\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9574 - acc: 0.7593 - val_loss: 0.9798 - val_acc: 0.7420\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9572 - acc: 0.7613 - val_loss: 0.9866 - val_acc: 0.7440\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9560 - acc: 0.7607 - val_loss: 0.9861 - val_acc: 0.7430\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9557 - acc: 0.7613 - val_loss: 0.9808 - val_acc: 0.7470\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9556 - acc: 0.7607 - val_loss: 0.9850 - val_acc: 0.7460\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9541 - acc: 0.7613 - val_loss: 0.9791 - val_acc: 0.7410\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9535 - acc: 0.7607 - val_loss: 0.9819 - val_acc: 0.7460\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9522 - acc: 0.7623 - val_loss: 0.9891 - val_acc: 0.7430\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9530 - acc: 0.7609 - val_loss: 0.9828 - val_acc: 0.7470\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9524 - acc: 0.7612 - val_loss: 0.9747 - val_acc: 0.7440\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9517 - acc: 0.7619 - val_loss: 0.9774 - val_acc: 0.7500\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9509 - acc: 0.7615 - val_loss: 0.9763 - val_acc: 0.7480\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9496 - acc: 0.7615 - val_loss: 0.9781 - val_acc: 0.7420\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9487 - acc: 0.7620 - val_loss: 0.9826 - val_acc: 0.7440\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9495 - acc: 0.7613 - val_loss: 0.9862 - val_acc: 0.7390\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9478 - acc: 0.7608 - val_loss: 0.9753 - val_acc: 0.7470\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9476 - acc: 0.7625 - val_loss: 0.9743 - val_acc: 0.7480\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9472 - acc: 0.7631 - val_loss: 0.9863 - val_acc: 0.7380\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9470 - acc: 0.7621 - val_loss: 0.9777 - val_acc: 0.7430\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9461 - acc: 0.7631 - val_loss: 0.9742 - val_acc: 0.7450\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9444 - acc: 0.7619 - val_loss: 0.9776 - val_acc: 0.7430\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9451 - acc: 0.7632 - val_loss: 0.9964 - val_acc: 0.7420\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9464 - acc: 0.7609 - val_loss: 0.9748 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9440 - acc: 0.7625 - val_loss: 0.9709 - val_acc: 0.7440\n",
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9428 - acc: 0.7631 - val_loss: 0.9729 - val_acc: 0.7430\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9425 - acc: 0.7639 - val_loss: 0.9768 - val_acc: 0.7440\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9414 - acc: 0.7629 - val_loss: 0.9732 - val_acc: 0.7460\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9401 - acc: 0.7624 - val_loss: 0.9733 - val_acc: 0.7490\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9417 - acc: 0.7637 - val_loss: 0.9699 - val_acc: 0.7500\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9400 - acc: 0.7637 - val_loss: 0.9691 - val_acc: 0.7460\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9398 - acc: 0.7637 - val_loss: 0.9733 - val_acc: 0.7490\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9399 - acc: 0.7641 - val_loss: 0.9692 - val_acc: 0.7440\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9381 - acc: 0.7660 - val_loss: 0.9757 - val_acc: 0.7440\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9380 - acc: 0.7635 - val_loss: 0.9797 - val_acc: 0.7490\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9376 - acc: 0.7633 - val_loss: 0.9774 - val_acc: 0.7410\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9380 - acc: 0.7639 - val_loss: 0.9698 - val_acc: 0.7490\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9375 - acc: 0.7635 - val_loss: 0.9673 - val_acc: 0.7450\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9360 - acc: 0.7633 - val_loss: 0.9632 - val_acc: 0.7470\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9344 - acc: 0.7640 - val_loss: 0.9802 - val_acc: 0.7420\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9349 - acc: 0.7621 - val_loss: 0.9719 - val_acc: 0.7490\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9336 - acc: 0.7671 - val_loss: 0.9697 - val_acc: 0.7490\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9342 - acc: 0.7637 - val_loss: 0.9614 - val_acc: 0.7500\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9331 - acc: 0.7620 - val_loss: 0.9673 - val_acc: 0.7490\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9328 - acc: 0.7641 - val_loss: 0.9598 - val_acc: 0.7480\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9309 - acc: 0.7656 - val_loss: 0.9651 - val_acc: 0.7470\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9309 - acc: 0.7657 - val_loss: 0.9612 - val_acc: 0.7460\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9306 - acc: 0.7653 - val_loss: 0.9715 - val_acc: 0.7490\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9309 - acc: 0.7652 - val_loss: 0.9599 - val_acc: 0.7500\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9296 - acc: 0.7640 - val_loss: 0.9574 - val_acc: 0.7460\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9294 - acc: 0.7667 - val_loss: 0.9615 - val_acc: 0.7470\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9290 - acc: 0.7649 - val_loss: 0.9774 - val_acc: 0.7420\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9287 - acc: 0.7668 - val_loss: 0.9589 - val_acc: 0.7450\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9274 - acc: 0.7651 - val_loss: 0.9645 - val_acc: 0.7480\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9271 - acc: 0.7645 - val_loss: 0.9647 - val_acc: 0.7520\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9271 - acc: 0.7652 - val_loss: 0.9573 - val_acc: 0.7480\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9272 - acc: 0.7640 - val_loss: 0.9566 - val_acc: 0.7500\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9246 - acc: 0.7651 - val_loss: 0.9571 - val_acc: 0.7440\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9248 - acc: 0.7660 - val_loss: 0.9808 - val_acc: 0.7440\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9255 - acc: 0.7659 - val_loss: 0.9615 - val_acc: 0.7490\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9237 - acc: 0.7675 - val_loss: 0.9603 - val_acc: 0.7470\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9233 - acc: 0.7656 - val_loss: 0.9572 - val_acc: 0.7490\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9236 - acc: 0.7649 - val_loss: 0.9661 - val_acc: 0.7490\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9235 - acc: 0.7653 - val_loss: 0.9571 - val_acc: 0.7470\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9218 - acc: 0.7652 - val_loss: 0.9567 - val_acc: 0.7500\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9210 - acc: 0.7665 - val_loss: 0.9537 - val_acc: 0.7470\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9213 - acc: 0.7656 - val_loss: 0.9576 - val_acc: 0.7470\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9224 - acc: 0.7633 - val_loss: 0.9582 - val_acc: 0.7510\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9196 - acc: 0.7669 - val_loss: 0.9619 - val_acc: 0.7440\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9202 - acc: 0.7672 - val_loss: 0.9518 - val_acc: 0.7500\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9182 - acc: 0.7681 - val_loss: 0.9629 - val_acc: 0.7490\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9182 - acc: 0.7669 - val_loss: 0.9508 - val_acc: 0.7490\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9174 - acc: 0.7669 - val_loss: 0.9643 - val_acc: 0.7470\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9187 - acc: 0.7673 - val_loss: 0.9751 - val_acc: 0.7430\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9194 - acc: 0.7659 - val_loss: 0.9492 - val_acc: 0.7460\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9158 - acc: 0.7688 - val_loss: 0.9552 - val_acc: 0.7430\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9160 - acc: 0.767 - 0s 43us/step - loss: 0.9163 - acc: 0.7679 - val_loss: 0.9654 - val_acc: 0.7510\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9163 - acc: 0.7648 - val_loss: 0.9518 - val_acc: 0.7530\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9166 - acc: 0.7673 - val_loss: 0.9465 - val_acc: 0.7470\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9153 - acc: 0.7671 - val_loss: 0.9700 - val_acc: 0.7450\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9133 - acc: 0.7675 - val_loss: 0.9527 - val_acc: 0.7440\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9135 - acc: 0.7680 - val_loss: 0.9530 - val_acc: 0.7440\n",
      "Epoch 412/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9135 - acc: 0.7685 - val_loss: 0.9517 - val_acc: 0.7470\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9128 - acc: 0.7677 - val_loss: 0.9504 - val_acc: 0.7550\n",
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9127 - acc: 0.7669 - val_loss: 0.9640 - val_acc: 0.7450\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9118 - acc: 0.7697 - val_loss: 0.9501 - val_acc: 0.7520\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9102 - acc: 0.7672 - val_loss: 0.9544 - val_acc: 0.7540\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9106 - acc: 0.7685 - val_loss: 0.9440 - val_acc: 0.7470\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9111 - acc: 0.7689 - val_loss: 0.9602 - val_acc: 0.7470\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9109 - acc: 0.7677 - val_loss: 0.9483 - val_acc: 0.7460\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9089 - acc: 0.7695 - val_loss: 0.9521 - val_acc: 0.7490\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9091 - acc: 0.7692 - val_loss: 0.9516 - val_acc: 0.7540\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9083 - acc: 0.7699 - val_loss: 0.9463 - val_acc: 0.7460\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9080 - acc: 0.7692 - val_loss: 0.9514 - val_acc: 0.7520\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9085 - acc: 0.7688 - val_loss: 0.9464 - val_acc: 0.7520\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9074 - acc: 0.7681 - val_loss: 0.9426 - val_acc: 0.7500\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9069 - acc: 0.7712 - val_loss: 0.9469 - val_acc: 0.7490\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9073 - acc: 0.7692 - val_loss: 0.9451 - val_acc: 0.7510\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9055 - acc: 0.7700 - val_loss: 0.9398 - val_acc: 0.7490\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9057 - acc: 0.7707 - val_loss: 0.9660 - val_acc: 0.7410\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9073 - acc: 0.7676 - val_loss: 0.9426 - val_acc: 0.7520\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9065 - acc: 0.7683 - val_loss: 0.9585 - val_acc: 0.7480\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.9048 - acc: 0.7696 - val_loss: 0.9527 - val_acc: 0.7480\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9044 - acc: 0.7693 - val_loss: 0.9417 - val_acc: 0.7470\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9030 - acc: 0.7721 - val_loss: 0.9433 - val_acc: 0.7460\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9036 - acc: 0.7695 - val_loss: 0.9533 - val_acc: 0.7460\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9042 - acc: 0.7707 - val_loss: 0.9458 - val_acc: 0.7530\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9042 - acc: 0.7695 - val_loss: 0.9440 - val_acc: 0.7580\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9020 - acc: 0.7709 - val_loss: 0.9439 - val_acc: 0.7570\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9007 - acc: 0.7711 - val_loss: 0.9357 - val_acc: 0.7490\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9015 - acc: 0.7700 - val_loss: 0.9394 - val_acc: 0.7490\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9009 - acc: 0.7711 - val_loss: 0.9380 - val_acc: 0.7470\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9003 - acc: 0.7723 - val_loss: 0.9385 - val_acc: 0.7470\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8993 - acc: 0.7709 - val_loss: 0.9382 - val_acc: 0.7480\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8995 - acc: 0.7705 - val_loss: 0.9459 - val_acc: 0.7480\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9014 - acc: 0.7701 - val_loss: 0.9428 - val_acc: 0.7440\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8987 - acc: 0.7701 - val_loss: 0.9371 - val_acc: 0.7530\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8990 - acc: 0.7703 - val_loss: 0.9394 - val_acc: 0.7560\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8982 - acc: 0.7729 - val_loss: 0.9337 - val_acc: 0.7490\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8964 - acc: 0.7727 - val_loss: 0.9394 - val_acc: 0.7450\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8969 - acc: 0.7689 - val_loss: 0.9344 - val_acc: 0.7520\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8968 - acc: 0.7725 - val_loss: 0.9466 - val_acc: 0.7510\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8964 - acc: 0.7728 - val_loss: 0.9319 - val_acc: 0.7470\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8957 - acc: 0.7716 - val_loss: 0.9363 - val_acc: 0.7510\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8966 - acc: 0.7717 - val_loss: 0.9444 - val_acc: 0.7560\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8955 - acc: 0.7725 - val_loss: 0.9388 - val_acc: 0.7490\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8960 - acc: 0.7720 - val_loss: 0.9344 - val_acc: 0.7480\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8942 - acc: 0.7724 - val_loss: 0.9418 - val_acc: 0.7500\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8951 - acc: 0.7700 - val_loss: 0.9509 - val_acc: 0.7410\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8949 - acc: 0.7703 - val_loss: 0.9320 - val_acc: 0.7500\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8939 - acc: 0.7717 - val_loss: 0.9406 - val_acc: 0.7460\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8946 - acc: 0.7725 - val_loss: 0.9379 - val_acc: 0.7590\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8940 - acc: 0.7700 - val_loss: 0.9326 - val_acc: 0.7450\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8934 - acc: 0.7716 - val_loss: 0.9345 - val_acc: 0.7470\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8919 - acc: 0.7716 - val_loss: 0.9304 - val_acc: 0.7490\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8908 - acc: 0.7731 - val_loss: 0.9346 - val_acc: 0.7580\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8895 - acc: 0.7731 - val_loss: 0.9425 - val_acc: 0.7510\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8912 - acc: 0.7737 - val_loss: 0.9383 - val_acc: 0.7490\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8923 - acc: 0.7705 - val_loss: 0.9312 - val_acc: 0.7510\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8911 - acc: 0.7752 - val_loss: 0.9309 - val_acc: 0.7490\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8896 - acc: 0.7716 - val_loss: 0.9292 - val_acc: 0.7490\n",
      "Epoch 471/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8903 - acc: 0.7737 - val_loss: 0.9358 - val_acc: 0.7540\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8891 - acc: 0.7723 - val_loss: 0.9324 - val_acc: 0.7470\n",
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8888 - acc: 0.7727 - val_loss: 0.9336 - val_acc: 0.7460\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8895 - acc: 0.7715 - val_loss: 0.9375 - val_acc: 0.7460\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8891 - acc: 0.7724 - val_loss: 0.9387 - val_acc: 0.7470\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8896 - acc: 0.7760 - val_loss: 0.9460 - val_acc: 0.7470\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8892 - acc: 0.7727 - val_loss: 0.9384 - val_acc: 0.7550\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8875 - acc: 0.7729 - val_loss: 0.9299 - val_acc: 0.7500\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8890 - acc: 0.7739 - val_loss: 0.9311 - val_acc: 0.7490\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8872 - acc: 0.7739 - val_loss: 0.9349 - val_acc: 0.7520\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8871 - acc: 0.7736 - val_loss: 0.9354 - val_acc: 0.7540\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8861 - acc: 0.7723 - val_loss: 0.9402 - val_acc: 0.7420\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8859 - acc: 0.7757 - val_loss: 0.9450 - val_acc: 0.7510\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8861 - acc: 0.7740 - val_loss: 0.9368 - val_acc: 0.7490\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8856 - acc: 0.7737 - val_loss: 0.9306 - val_acc: 0.7470\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8854 - acc: 0.7756 - val_loss: 0.9536 - val_acc: 0.7390\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8866 - acc: 0.7740 - val_loss: 0.9566 - val_acc: 0.7380\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8868 - acc: 0.7739 - val_loss: 0.9261 - val_acc: 0.7520\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8829 - acc: 0.7752 - val_loss: 0.9344 - val_acc: 0.7560\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8831 - acc: 0.7741 - val_loss: 0.9331 - val_acc: 0.7500\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8846 - acc: 0.7753 - val_loss: 0.9290 - val_acc: 0.7520\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8840 - acc: 0.7745 - val_loss: 0.9361 - val_acc: 0.7480\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8831 - acc: 0.7735 - val_loss: 0.9340 - val_acc: 0.7440\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8813 - acc: 0.7760 - val_loss: 0.9386 - val_acc: 0.7470\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8834 - acc: 0.7739 - val_loss: 0.9243 - val_acc: 0.7470\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8821 - acc: 0.7763 - val_loss: 0.9263 - val_acc: 0.7500\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8805 - acc: 0.7764 - val_loss: 0.9277 - val_acc: 0.7540\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8831 - acc: 0.7748 - val_loss: 0.9337 - val_acc: 0.7470\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8841 - acc: 0.7751 - val_loss: 0.9313 - val_acc: 0.7500\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8805 - acc: 0.7764 - val_loss: 0.9301 - val_acc: 0.7480\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8811 - acc: 0.7749 - val_loss: 0.9303 - val_acc: 0.7470\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8807 - acc: 0.7759 - val_loss: 0.9240 - val_acc: 0.7510\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8798 - acc: 0.7767 - val_loss: 0.9280 - val_acc: 0.7480\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8800 - acc: 0.7753 - val_loss: 0.9203 - val_acc: 0.7490\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8796 - acc: 0.7739 - val_loss: 1.0191 - val_acc: 0.7120\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8815 - acc: 0.7745 - val_loss: 0.9302 - val_acc: 0.7490\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8797 - acc: 0.7769 - val_loss: 0.9255 - val_acc: 0.7540\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8790 - acc: 0.7741 - val_loss: 0.9277 - val_acc: 0.7460\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8792 - acc: 0.7748 - val_loss: 0.9220 - val_acc: 0.7500\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8775 - acc: 0.7749 - val_loss: 0.9226 - val_acc: 0.7520\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8779 - acc: 0.7765 - val_loss: 0.9368 - val_acc: 0.7490\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8780 - acc: 0.7759 - val_loss: 0.9242 - val_acc: 0.7560\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8762 - acc: 0.7775 - val_loss: 0.9455 - val_acc: 0.7410\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8772 - acc: 0.7757 - val_loss: 0.9255 - val_acc: 0.7510\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8757 - acc: 0.7752 - val_loss: 0.9393 - val_acc: 0.7430\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8770 - acc: 0.7777 - val_loss: 0.9320 - val_acc: 0.7510\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8747 - acc: 0.7768 - val_loss: 0.9276 - val_acc: 0.7500\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8758 - acc: 0.7776 - val_loss: 0.9325 - val_acc: 0.7520\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8761 - acc: 0.7749 - val_loss: 0.9276 - val_acc: 0.7500\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8765 - acc: 0.7765 - val_loss: 0.9680 - val_acc: 0.7350\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8773 - acc: 0.7757 - val_loss: 0.9256 - val_acc: 0.7580\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8741 - acc: 0.7757 - val_loss: 0.9486 - val_acc: 0.7530\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8757 - acc: 0.7756 - val_loss: 0.9229 - val_acc: 0.7520\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8730 - acc: 0.7768 - val_loss: 0.9211 - val_acc: 0.7490\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8756 - acc: 0.7763 - val_loss: 0.9259 - val_acc: 0.7480\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8780 - acc: 0.7764 - val_loss: 0.9252 - val_acc: 0.7570\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8750 - acc: 0.7752 - val_loss: 0.9175 - val_acc: 0.7520\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8730 - acc: 0.7780 - val_loss: 0.9305 - val_acc: 0.7490\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8731 - acc: 0.7784 - val_loss: 0.9216 - val_acc: 0.7520\n",
      "Epoch 530/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8718 - acc: 0.7777 - val_loss: 0.9278 - val_acc: 0.7480\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8722 - acc: 0.7791 - val_loss: 0.9212 - val_acc: 0.7480\n",
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8716 - acc: 0.7797 - val_loss: 0.9204 - val_acc: 0.7510\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8746 - acc: 0.7763 - val_loss: 0.9293 - val_acc: 0.7600\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8701 - acc: 0.7801 - val_loss: 0.9259 - val_acc: 0.7570\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8716 - acc: 0.7780 - val_loss: 0.9216 - val_acc: 0.7560\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8722 - acc: 0.7799 - val_loss: 0.9191 - val_acc: 0.7450\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8703 - acc: 0.7772 - val_loss: 0.9192 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8714 - acc: 0.7764 - val_loss: 0.9368 - val_acc: 0.7550\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8711 - acc: 0.7784 - val_loss: 0.9357 - val_acc: 0.7440\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8707 - acc: 0.7791 - val_loss: 0.9336 - val_acc: 0.7440\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8743 - acc: 0.7761 - val_loss: 0.9176 - val_acc: 0.7560\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8704 - acc: 0.7776 - val_loss: 0.9236 - val_acc: 0.7530\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8695 - acc: 0.7795 - val_loss: 0.9419 - val_acc: 0.7410\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8713 - acc: 0.7777 - val_loss: 0.9175 - val_acc: 0.7510\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8714 - acc: 0.7787 - val_loss: 0.9214 - val_acc: 0.7510\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8711 - acc: 0.7772 - val_loss: 0.9403 - val_acc: 0.7540\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8696 - acc: 0.7785 - val_loss: 0.9345 - val_acc: 0.7400\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8694 - acc: 0.7769 - val_loss: 0.9181 - val_acc: 0.7480\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8691 - acc: 0.7785 - val_loss: 0.9461 - val_acc: 0.7370\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8712 - acc: 0.7771 - val_loss: 0.9843 - val_acc: 0.7250\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8696 - acc: 0.7783 - val_loss: 0.9585 - val_acc: 0.7340\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8687 - acc: 0.7784 - val_loss: 0.9217 - val_acc: 0.7450\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8681 - acc: 0.7787 - val_loss: 0.9163 - val_acc: 0.7530\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8675 - acc: 0.7773 - val_loss: 0.9271 - val_acc: 0.7540\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8669 - acc: 0.7776 - val_loss: 0.9155 - val_acc: 0.7540\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8652 - acc: 0.7784 - val_loss: 0.9280 - val_acc: 0.7490\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8668 - acc: 0.7784 - val_loss: 0.9513 - val_acc: 0.7450\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8693 - acc: 0.7787 - val_loss: 0.9189 - val_acc: 0.7440\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8657 - acc: 0.7780 - val_loss: 0.9242 - val_acc: 0.7480\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8643 - acc: 0.7811 - val_loss: 0.9267 - val_acc: 0.7430\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8661 - acc: 0.7803 - val_loss: 0.9280 - val_acc: 0.7560\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8652 - acc: 0.7807 - val_loss: 0.9702 - val_acc: 0.7300\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8650 - acc: 0.7809 - val_loss: 0.9477 - val_acc: 0.7490\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8658 - acc: 0.7793 - val_loss: 0.9196 - val_acc: 0.7590\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8650 - acc: 0.7779 - val_loss: 0.9157 - val_acc: 0.7490\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8646 - acc: 0.7796 - val_loss: 0.9218 - val_acc: 0.7590\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8647 - acc: 0.7800 - val_loss: 0.9199 - val_acc: 0.7540\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8637 - acc: 0.7792 - val_loss: 0.9179 - val_acc: 0.7490\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8629 - acc: 0.7811 - val_loss: 0.9146 - val_acc: 0.7580\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8644 - acc: 0.7807 - val_loss: 0.9242 - val_acc: 0.7500\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8634 - acc: 0.7781 - val_loss: 0.9187 - val_acc: 0.7510\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8638 - acc: 0.7820 - val_loss: 0.9207 - val_acc: 0.7570\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8620 - acc: 0.7791 - val_loss: 0.9130 - val_acc: 0.7560\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8614 - acc: 0.7816 - val_loss: 0.9179 - val_acc: 0.7500\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8616 - acc: 0.7799 - val_loss: 0.9121 - val_acc: 0.7570\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8627 - acc: 0.7801 - val_loss: 0.9295 - val_acc: 0.7490\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8632 - acc: 0.7807 - val_loss: 0.9148 - val_acc: 0.7480\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8610 - acc: 0.7801 - val_loss: 0.9252 - val_acc: 0.7460\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8590 - acc: 0.7815 - val_loss: 0.9111 - val_acc: 0.7570\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8602 - acc: 0.7801 - val_loss: 0.9229 - val_acc: 0.7480\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8632 - acc: 0.7787 - val_loss: 0.9149 - val_acc: 0.7550\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8595 - acc: 0.7816 - val_loss: 0.9139 - val_acc: 0.7550\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8612 - acc: 0.7816 - val_loss: 0.9118 - val_acc: 0.7550\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8627 - acc: 0.7797 - val_loss: 0.9141 - val_acc: 0.7590\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8600 - acc: 0.7825 - val_loss: 0.9315 - val_acc: 0.7510\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8590 - acc: 0.7805 - val_loss: 0.9302 - val_acc: 0.7560\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8601 - acc: 0.7788 - val_loss: 0.9203 - val_acc: 0.7580\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8588 - acc: 0.7805 - val_loss: 0.9131 - val_acc: 0.7450\n",
      "Epoch 589/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8579 - acc: 0.7821 - val_loss: 0.9215 - val_acc: 0.7570\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8581 - acc: 0.7812 - val_loss: 0.9196 - val_acc: 0.7480\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8592 - acc: 0.7799 - val_loss: 0.9155 - val_acc: 0.7500\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8597 - acc: 0.7809 - val_loss: 0.9237 - val_acc: 0.7460\n",
      "Epoch 593/1000\n",
      "2304/7500 [========>.....................] - ETA: 0s - loss: 0.8566 - acc: 0.7904"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-fc6c720fc893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     validation_data=(val, label_val))\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8237653533299764, 0.7967999999682108]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.966706668694814, 0.7499999998410543]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 2.0228 - acc: 0.1372 - val_loss: 1.9610 - val_acc: 0.1380\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9760 - acc: 0.1439 - val_loss: 1.9430 - val_acc: 0.1630\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9560 - acc: 0.1548 - val_loss: 1.9328 - val_acc: 0.1790\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9462 - acc: 0.1639 - val_loss: 1.9249 - val_acc: 0.1890\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9412 - acc: 0.1661 - val_loss: 1.9189 - val_acc: 0.2040\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9261 - acc: 0.1800 - val_loss: 1.9124 - val_acc: 0.2090\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9231 - acc: 0.1896 - val_loss: 1.9062 - val_acc: 0.2310\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9152 - acc: 0.1916 - val_loss: 1.8993 - val_acc: 0.2430\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9105 - acc: 0.1981 - val_loss: 1.8928 - val_acc: 0.2540\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9032 - acc: 0.2036 - val_loss: 1.8854 - val_acc: 0.2590\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8927 - acc: 0.2129 - val_loss: 1.8773 - val_acc: 0.2640\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8881 - acc: 0.2221 - val_loss: 1.8688 - val_acc: 0.2660\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8835 - acc: 0.2249 - val_loss: 1.8599 - val_acc: 0.2720\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8740 - acc: 0.2337 - val_loss: 1.8489 - val_acc: 0.2840\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8637 - acc: 0.2427 - val_loss: 1.8371 - val_acc: 0.2930\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8544 - acc: 0.2432 - val_loss: 1.8235 - val_acc: 0.3030\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8401 - acc: 0.2505 - val_loss: 1.8080 - val_acc: 0.3100\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8334 - acc: 0.2677 - val_loss: 1.7913 - val_acc: 0.3120\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8201 - acc: 0.2657 - val_loss: 1.7722 - val_acc: 0.3190\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8009 - acc: 0.2712 - val_loss: 1.7512 - val_acc: 0.3240\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7911 - acc: 0.2812 - val_loss: 1.7285 - val_acc: 0.3310\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7735 - acc: 0.2939 - val_loss: 1.7043 - val_acc: 0.3420\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7500 - acc: 0.3079 - val_loss: 1.6776 - val_acc: 0.3510\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7402 - acc: 0.3083 - val_loss: 1.6486 - val_acc: 0.3800\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7199 - acc: 0.3201 - val_loss: 1.6207 - val_acc: 0.3930\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6974 - acc: 0.3253 - val_loss: 1.5931 - val_acc: 0.4070\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6781 - acc: 0.3377 - val_loss: 1.5651 - val_acc: 0.4270\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6711 - acc: 0.3471 - val_loss: 1.5406 - val_acc: 0.4440\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6474 - acc: 0.3469 - val_loss: 1.5145 - val_acc: 0.4510\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6281 - acc: 0.3521 - val_loss: 1.4876 - val_acc: 0.4610\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6150 - acc: 0.3639 - val_loss: 1.4634 - val_acc: 0.4750\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5985 - acc: 0.3748 - val_loss: 1.4395 - val_acc: 0.4940\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5763 - acc: 0.3808 - val_loss: 1.4134 - val_acc: 0.5050\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5539 - acc: 0.3969 - val_loss: 1.3892 - val_acc: 0.5280\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5548 - acc: 0.3883 - val_loss: 1.3709 - val_acc: 0.5280\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5287 - acc: 0.4059 - val_loss: 1.3478 - val_acc: 0.5470\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5163 - acc: 0.4099 - val_loss: 1.3288 - val_acc: 0.5560\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5083 - acc: 0.4137 - val_loss: 1.3094 - val_acc: 0.5720\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4847 - acc: 0.4319 - val_loss: 1.2877 - val_acc: 0.5770\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4855 - acc: 0.4204 - val_loss: 1.2714 - val_acc: 0.5920\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4642 - acc: 0.4336 - val_loss: 1.2518 - val_acc: 0.6070\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4565 - acc: 0.4380 - val_loss: 1.2329 - val_acc: 0.6190\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4303 - acc: 0.4479 - val_loss: 1.2134 - val_acc: 0.6380\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4283 - acc: 0.4561 - val_loss: 1.1985 - val_acc: 0.6450\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3998 - acc: 0.4632 - val_loss: 1.1791 - val_acc: 0.6420\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3886 - acc: 0.4664 - val_loss: 1.1616 - val_acc: 0.6460\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3729 - acc: 0.4784 - val_loss: 1.1441 - val_acc: 0.6540\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3671 - acc: 0.4807 - val_loss: 1.1300 - val_acc: 0.6640\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3640 - acc: 0.4808 - val_loss: 1.1187 - val_acc: 0.6570\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3373 - acc: 0.4841 - val_loss: 1.0995 - val_acc: 0.6660\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3246 - acc: 0.4999 - val_loss: 1.0832 - val_acc: 0.6730\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3099 - acc: 0.5103 - val_loss: 1.0693 - val_acc: 0.6760\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3033 - acc: 0.5064 - val_loss: 1.0562 - val_acc: 0.6850\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2944 - acc: 0.5116 - val_loss: 1.0446 - val_acc: 0.6820\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2866 - acc: 0.5151 - val_loss: 1.0344 - val_acc: 0.6850\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2655 - acc: 0.5233 - val_loss: 1.0180 - val_acc: 0.6880\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2563 - acc: 0.5239 - val_loss: 1.0041 - val_acc: 0.6920\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2481 - acc: 0.5256 - val_loss: 0.9925 - val_acc: 0.6950\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2476 - acc: 0.5280 - val_loss: 0.9800 - val_acc: 0.6940\n",
      "Epoch 60/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2354 - acc: 0.5300 - val_loss: 0.9684 - val_acc: 0.7050\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2150 - acc: 0.5400 - val_loss: 0.9589 - val_acc: 0.7120\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2109 - acc: 0.5513 - val_loss: 0.9459 - val_acc: 0.7030\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2099 - acc: 0.5472 - val_loss: 0.9360 - val_acc: 0.7070\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1975 - acc: 0.5485 - val_loss: 0.9275 - val_acc: 0.7140\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1849 - acc: 0.5585 - val_loss: 0.9184 - val_acc: 0.7120\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1837 - acc: 0.5669 - val_loss: 0.9071 - val_acc: 0.7230\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1751 - acc: 0.5639 - val_loss: 0.8986 - val_acc: 0.7200\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1549 - acc: 0.5747 - val_loss: 0.8873 - val_acc: 0.7260\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1635 - acc: 0.5651 - val_loss: 0.8807 - val_acc: 0.7260\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1557 - acc: 0.5672 - val_loss: 0.8725 - val_acc: 0.7280\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1337 - acc: 0.5803 - val_loss: 0.8605 - val_acc: 0.7310\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1170 - acc: 0.5859 - val_loss: 0.8531 - val_acc: 0.7290\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1179 - acc: 0.5797 - val_loss: 0.8437 - val_acc: 0.7330\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1314 - acc: 0.5715 - val_loss: 0.8400 - val_acc: 0.7340\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0935 - acc: 0.5948 - val_loss: 0.8310 - val_acc: 0.7370\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0940 - acc: 0.5903 - val_loss: 0.8213 - val_acc: 0.7360\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0864 - acc: 0.5919 - val_loss: 0.8140 - val_acc: 0.7370\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0872 - acc: 0.5936 - val_loss: 0.8073 - val_acc: 0.7370\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0767 - acc: 0.5975 - val_loss: 0.7990 - val_acc: 0.7370\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0606 - acc: 0.6045 - val_loss: 0.7912 - val_acc: 0.7390\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0702 - acc: 0.6021 - val_loss: 0.7851 - val_acc: 0.7400\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0764 - acc: 0.5968 - val_loss: 0.7811 - val_acc: 0.7430\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0551 - acc: 0.6103 - val_loss: 0.7777 - val_acc: 0.7410\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0540 - acc: 0.6044 - val_loss: 0.7700 - val_acc: 0.7450\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0394 - acc: 0.6139 - val_loss: 0.7644 - val_acc: 0.7470\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0397 - acc: 0.6067 - val_loss: 0.7589 - val_acc: 0.7470\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0272 - acc: 0.6219 - val_loss: 0.7505 - val_acc: 0.7510\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0158 - acc: 0.6245 - val_loss: 0.7446 - val_acc: 0.7460\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0275 - acc: 0.6136 - val_loss: 0.7412 - val_acc: 0.7500\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0228 - acc: 0.6173 - val_loss: 0.7377 - val_acc: 0.7490\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0199 - acc: 0.6184 - val_loss: 0.7353 - val_acc: 0.7500\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0134 - acc: 0.6215 - val_loss: 0.7303 - val_acc: 0.7530\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0174 - acc: 0.6208 - val_loss: 0.7271 - val_acc: 0.7500\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0023 - acc: 0.6265 - val_loss: 0.7241 - val_acc: 0.7500\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0029 - acc: 0.6220 - val_loss: 0.7190 - val_acc: 0.7540\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9879 - acc: 0.6327 - val_loss: 0.7127 - val_acc: 0.7560\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9865 - acc: 0.6252 - val_loss: 0.7100 - val_acc: 0.7550\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9628 - acc: 0.6372 - val_loss: 0.7027 - val_acc: 0.7590\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9743 - acc: 0.6404 - val_loss: 0.7010 - val_acc: 0.7580\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9698 - acc: 0.6383 - val_loss: 0.6961 - val_acc: 0.7580\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9758 - acc: 0.6328 - val_loss: 0.6926 - val_acc: 0.7590\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9662 - acc: 0.6391 - val_loss: 0.6881 - val_acc: 0.7580\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9672 - acc: 0.6409 - val_loss: 0.6854 - val_acc: 0.7550\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9480 - acc: 0.6501 - val_loss: 0.6818 - val_acc: 0.7640\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9476 - acc: 0.6475 - val_loss: 0.6776 - val_acc: 0.7610\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9526 - acc: 0.6485 - val_loss: 0.6744 - val_acc: 0.7600\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9478 - acc: 0.6456 - val_loss: 0.6734 - val_acc: 0.7620\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9373 - acc: 0.6532 - val_loss: 0.6690 - val_acc: 0.7600\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9404 - acc: 0.6469 - val_loss: 0.6653 - val_acc: 0.7620\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9366 - acc: 0.6524 - val_loss: 0.6629 - val_acc: 0.7620\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9141 - acc: 0.6569 - val_loss: 0.6577 - val_acc: 0.7650\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9396 - acc: 0.6484 - val_loss: 0.6591 - val_acc: 0.7620\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9192 - acc: 0.6569 - val_loss: 0.6553 - val_acc: 0.7630\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9250 - acc: 0.6605 - val_loss: 0.6542 - val_acc: 0.7620\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9224 - acc: 0.6533 - val_loss: 0.6496 - val_acc: 0.7640\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9200 - acc: 0.6568 - val_loss: 0.6510 - val_acc: 0.7640\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9003 - acc: 0.6680 - val_loss: 0.6470 - val_acc: 0.7610\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9010 - acc: 0.6671 - val_loss: 0.6434 - val_acc: 0.7660\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9034 - acc: 0.6628 - val_loss: 0.6428 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9110 - acc: 0.6584 - val_loss: 0.6407 - val_acc: 0.7640\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8934 - acc: 0.6695 - val_loss: 0.6380 - val_acc: 0.7670\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8921 - acc: 0.6660 - val_loss: 0.6374 - val_acc: 0.7640\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8894 - acc: 0.6664 - val_loss: 0.6322 - val_acc: 0.7660\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8856 - acc: 0.6701 - val_loss: 0.6285 - val_acc: 0.7670\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8889 - acc: 0.6688 - val_loss: 0.6283 - val_acc: 0.7700\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8769 - acc: 0.6671 - val_loss: 0.6276 - val_acc: 0.7680\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8794 - acc: 0.6720 - val_loss: 0.6248 - val_acc: 0.7670\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8593 - acc: 0.6759 - val_loss: 0.6234 - val_acc: 0.7690\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8801 - acc: 0.6691 - val_loss: 0.6223 - val_acc: 0.7690\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8658 - acc: 0.6731 - val_loss: 0.6190 - val_acc: 0.7720\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8869 - acc: 0.6708 - val_loss: 0.6189 - val_acc: 0.7750\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8563 - acc: 0.6824 - val_loss: 0.6160 - val_acc: 0.7750\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8640 - acc: 0.6839 - val_loss: 0.6151 - val_acc: 0.7740\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8484 - acc: 0.6855 - val_loss: 0.6113 - val_acc: 0.7710\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8609 - acc: 0.6819 - val_loss: 0.6112 - val_acc: 0.7730\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8558 - acc: 0.6849 - val_loss: 0.6074 - val_acc: 0.7730\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8475 - acc: 0.6873 - val_loss: 0.6076 - val_acc: 0.7700\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8470 - acc: 0.6843 - val_loss: 0.6069 - val_acc: 0.7720\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8506 - acc: 0.6827 - val_loss: 0.6039 - val_acc: 0.7750\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8442 - acc: 0.6788 - val_loss: 0.6027 - val_acc: 0.7720\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8418 - acc: 0.6872 - val_loss: 0.6037 - val_acc: 0.7760\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8406 - acc: 0.6839 - val_loss: 0.6015 - val_acc: 0.7760\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8358 - acc: 0.6884 - val_loss: 0.5979 - val_acc: 0.7790\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8416 - acc: 0.6879 - val_loss: 0.5957 - val_acc: 0.7770\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8329 - acc: 0.6801 - val_loss: 0.5942 - val_acc: 0.7750\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8179 - acc: 0.6956 - val_loss: 0.5947 - val_acc: 0.7760\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8390 - acc: 0.6831 - val_loss: 0.5946 - val_acc: 0.7770\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8243 - acc: 0.6896 - val_loss: 0.5922 - val_acc: 0.7760\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8181 - acc: 0.6913 - val_loss: 0.5918 - val_acc: 0.7810\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8336 - acc: 0.6841 - val_loss: 0.5914 - val_acc: 0.7810\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8093 - acc: 0.6941 - val_loss: 0.5911 - val_acc: 0.7800\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.6928 - val_loss: 0.5864 - val_acc: 0.7850\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8137 - acc: 0.7011 - val_loss: 0.5850 - val_acc: 0.7760\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8236 - acc: 0.6940 - val_loss: 0.5852 - val_acc: 0.7820\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8231 - acc: 0.6897 - val_loss: 0.5852 - val_acc: 0.7830\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8060 - acc: 0.7013 - val_loss: 0.5845 - val_acc: 0.7790\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7927 - acc: 0.7033 - val_loss: 0.5826 - val_acc: 0.7810\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7906 - acc: 0.7007 - val_loss: 0.5800 - val_acc: 0.7840\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8047 - acc: 0.6956 - val_loss: 0.5789 - val_acc: 0.7820\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8041 - acc: 0.7039 - val_loss: 0.5785 - val_acc: 0.7840\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7914 - acc: 0.7107 - val_loss: 0.5778 - val_acc: 0.7770\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7874 - acc: 0.7096 - val_loss: 0.5766 - val_acc: 0.7780\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7860 - acc: 0.7115 - val_loss: 0.5753 - val_acc: 0.7880\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7962 - acc: 0.7027 - val_loss: 0.5772 - val_acc: 0.7800\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7840 - acc: 0.7115 - val_loss: 0.5736 - val_acc: 0.7850\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7821 - acc: 0.7073 - val_loss: 0.5730 - val_acc: 0.7820\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7884 - acc: 0.7077 - val_loss: 0.5700 - val_acc: 0.7840\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7944 - acc: 0.7095 - val_loss: 0.5711 - val_acc: 0.7820\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7728 - acc: 0.7117 - val_loss: 0.5682 - val_acc: 0.7810\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7821 - acc: 0.7048 - val_loss: 0.5666 - val_acc: 0.7840\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7830 - acc: 0.7076 - val_loss: 0.5673 - val_acc: 0.7840\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7835 - acc: 0.7121 - val_loss: 0.5684 - val_acc: 0.7830\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7745 - acc: 0.7136 - val_loss: 0.5668 - val_acc: 0.7850\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7770 - acc: 0.7079 - val_loss: 0.5686 - val_acc: 0.7810\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7860 - acc: 0.7053 - val_loss: 0.5658 - val_acc: 0.7860\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7738 - acc: 0.7079 - val_loss: 0.5644 - val_acc: 0.7880\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7666 - acc: 0.7068 - val_loss: 0.5638 - val_acc: 0.7820\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7707 - acc: 0.7079 - val_loss: 0.5627 - val_acc: 0.7840\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7840 - acc: 0.7064 - val_loss: 0.5618 - val_acc: 0.7870\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7650 - acc: 0.7151 - val_loss: 0.5598 - val_acc: 0.7850\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7661 - acc: 0.7088 - val_loss: 0.5599 - val_acc: 0.7860\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7444 - acc: 0.7239 - val_loss: 0.5568 - val_acc: 0.7870\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7523 - acc: 0.7149 - val_loss: 0.5578 - val_acc: 0.7830\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7616 - acc: 0.7185 - val_loss: 0.5593 - val_acc: 0.7850\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7566 - acc: 0.7108 - val_loss: 0.5588 - val_acc: 0.7860\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7467 - acc: 0.7197 - val_loss: 0.5590 - val_acc: 0.7830\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7537 - acc: 0.7157 - val_loss: 0.5566 - val_acc: 0.7800\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7494 - acc: 0.7199 - val_loss: 0.5566 - val_acc: 0.7830\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7526 - acc: 0.7203 - val_loss: 0.5554 - val_acc: 0.7880\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7362 - acc: 0.7256 - val_loss: 0.5528 - val_acc: 0.7860\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7464 - acc: 0.7181 - val_loss: 0.5523 - val_acc: 0.7850\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7515 - acc: 0.7141 - val_loss: 0.5511 - val_acc: 0.7850\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7432 - acc: 0.7191 - val_loss: 0.5511 - val_acc: 0.7880\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7388 - acc: 0.7232 - val_loss: 0.5523 - val_acc: 0.7850\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7412 - acc: 0.7212 - val_loss: 0.5543 - val_acc: 0.7840\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7288 - acc: 0.7279 - val_loss: 0.5510 - val_acc: 0.7830\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7335 - acc: 0.7285 - val_loss: 0.5509 - val_acc: 0.7820\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7255 - acc: 0.7292 - val_loss: 0.5480 - val_acc: 0.7840\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7341 - acc: 0.7236 - val_loss: 0.5490 - val_acc: 0.7830\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7398 - acc: 0.7244 - val_loss: 0.5489 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 66us/step\n",
      "1500/1500 [==============================] - 0s 125us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8523120937665304, 0.7848000000317892]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9051667850812276, 0.7579999995231629]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 74us/step - loss: 1.9311 - acc: 0.1858 - val_loss: 1.9023 - val_acc: 0.2300\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 1.8675 - acc: 0.2539 - val_loss: 1.8204 - val_acc: 0.2913\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 1.7611 - acc: 0.3228 - val_loss: 1.6881 - val_acc: 0.3587\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 1.6127 - acc: 0.3931 - val_loss: 1.5285 - val_acc: 0.4453\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 1.4532 - acc: 0.4689 - val_loss: 1.3715 - val_acc: 0.5187\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 1.3036 - acc: 0.5495 - val_loss: 1.2315 - val_acc: 0.5937\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 1.1729 - acc: 0.6188 - val_loss: 1.1121 - val_acc: 0.6413\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 1.0609 - acc: 0.6640 - val_loss: 1.0115 - val_acc: 0.6750\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.9663 - acc: 0.6931 - val_loss: 0.9272 - val_acc: 0.6947\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.8883 - acc: 0.7118 - val_loss: 0.8605 - val_acc: 0.7120\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.8256 - acc: 0.7270 - val_loss: 0.8073 - val_acc: 0.7240\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.7763 - acc: 0.7369 - val_loss: 0.7663 - val_acc: 0.7327\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.7378 - acc: 0.7468 - val_loss: 0.7357 - val_acc: 0.7373\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.7069 - acc: 0.7552 - val_loss: 0.7109 - val_acc: 0.7463\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.6823 - acc: 0.7602 - val_loss: 0.6926 - val_acc: 0.7487\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.6620 - acc: 0.7663 - val_loss: 0.6771 - val_acc: 0.7563\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.6447 - acc: 0.7715 - val_loss: 0.6639 - val_acc: 0.7597\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.6297 - acc: 0.7769 - val_loss: 0.6522 - val_acc: 0.7640\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.6162 - acc: 0.7808 - val_loss: 0.6428 - val_acc: 0.7640\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.6042 - acc: 0.7849 - val_loss: 0.6356 - val_acc: 0.7700\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5932 - acc: 0.7884 - val_loss: 0.6278 - val_acc: 0.7717\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5832 - acc: 0.7933 - val_loss: 0.6201 - val_acc: 0.7730\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.5736 - acc: 0.7954 - val_loss: 0.6163 - val_acc: 0.7763\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.5653 - acc: 0.7980 - val_loss: 0.6092 - val_acc: 0.7770\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.5569 - acc: 0.8015 - val_loss: 0.6046 - val_acc: 0.7783\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5494 - acc: 0.8031 - val_loss: 0.5999 - val_acc: 0.7813\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5420 - acc: 0.8061 - val_loss: 0.5947 - val_acc: 0.7840\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5350 - acc: 0.8088 - val_loss: 0.5911 - val_acc: 0.7850\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.5286 - acc: 0.8111 - val_loss: 0.5880 - val_acc: 0.7887\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5224 - acc: 0.8145 - val_loss: 0.5831 - val_acc: 0.7897\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5167 - acc: 0.8155 - val_loss: 0.5800 - val_acc: 0.7910\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.5108 - acc: 0.8176 - val_loss: 0.5785 - val_acc: 0.7957\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5055 - acc: 0.8198 - val_loss: 0.5749 - val_acc: 0.7970\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5002 - acc: 0.8210 - val_loss: 0.5717 - val_acc: 0.7960\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4953 - acc: 0.8240 - val_loss: 0.5717 - val_acc: 0.8003\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.4904 - acc: 0.8269 - val_loss: 0.5667 - val_acc: 0.7970\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4858 - acc: 0.8265 - val_loss: 0.5646 - val_acc: 0.8013\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4813 - acc: 0.8279 - val_loss: 0.5626 - val_acc: 0.8023\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4770 - acc: 0.8303 - val_loss: 0.5605 - val_acc: 0.8013\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4725 - acc: 0.8314 - val_loss: 0.5589 - val_acc: 0.8043\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4686 - acc: 0.8340 - val_loss: 0.5614 - val_acc: 0.8030\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.4650 - acc: 0.8354 - val_loss: 0.5554 - val_acc: 0.8053\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4610 - acc: 0.8365 - val_loss: 0.5545 - val_acc: 0.8053\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4574 - acc: 0.8378 - val_loss: 0.5528 - val_acc: 0.8053\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4535 - acc: 0.8386 - val_loss: 0.5507 - val_acc: 0.8103\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4504 - acc: 0.8400 - val_loss: 0.5499 - val_acc: 0.8097\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4468 - acc: 0.8406 - val_loss: 0.5503 - val_acc: 0.8057\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4434 - acc: 0.8443 - val_loss: 0.5480 - val_acc: 0.8107\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.4405 - acc: 0.8438 - val_loss: 0.5475 - val_acc: 0.8100\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4374 - acc: 0.8458 - val_loss: 0.5456 - val_acc: 0.8133\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4344 - acc: 0.8471 - val_loss: 0.5462 - val_acc: 0.8133\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.4313 - acc: 0.8481 - val_loss: 0.5439 - val_acc: 0.8137\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.4283 - acc: 0.8497 - val_loss: 0.5445 - val_acc: 0.8110\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.4257 - acc: 0.8495 - val_loss: 0.5435 - val_acc: 0.8113\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4229 - acc: 0.8513 - val_loss: 0.5425 - val_acc: 0.8153\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4202 - acc: 0.8525 - val_loss: 0.5435 - val_acc: 0.8120\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4176 - acc: 0.8535 - val_loss: 0.5428 - val_acc: 0.8120\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.4149 - acc: 0.8546 - val_loss: 0.5412 - val_acc: 0.8147\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.4125 - acc: 0.8554 - val_loss: 0.5409 - val_acc: 0.8167\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4099 - acc: 0.8556 - val_loss: 0.5400 - val_acc: 0.8177\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4075 - acc: 0.8574 - val_loss: 0.5403 - val_acc: 0.8167\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4048 - acc: 0.8585 - val_loss: 0.5401 - val_acc: 0.8163\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4026 - acc: 0.8594 - val_loss: 0.5388 - val_acc: 0.8167\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4003 - acc: 0.8603 - val_loss: 0.5392 - val_acc: 0.8157\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3982 - acc: 0.8613 - val_loss: 0.5383 - val_acc: 0.8150\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 2s 45us/step - loss: 0.3960 - acc: 0.8616 - val_loss: 0.5387 - val_acc: 0.8173\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3936 - acc: 0.8621 - val_loss: 0.5389 - val_acc: 0.8170\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3915 - acc: 0.8636 - val_loss: 0.5381 - val_acc: 0.8157\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3893 - acc: 0.8647 - val_loss: 0.5394 - val_acc: 0.8167\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3871 - acc: 0.8655 - val_loss: 0.5380 - val_acc: 0.8160\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3852 - acc: 0.8660 - val_loss: 0.5394 - val_acc: 0.8180\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3835 - acc: 0.8661 - val_loss: 0.5383 - val_acc: 0.8190\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3813 - acc: 0.8668 - val_loss: 0.5387 - val_acc: 0.8167\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3792 - acc: 0.8674 - val_loss: 0.5375 - val_acc: 0.8167\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3775 - acc: 0.8688 - val_loss: 0.5396 - val_acc: 0.8173\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3752 - acc: 0.8691 - val_loss: 0.5381 - val_acc: 0.8157\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3735 - acc: 0.8699 - val_loss: 0.5380 - val_acc: 0.8170\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3717 - acc: 0.8706 - val_loss: 0.5400 - val_acc: 0.8173\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3699 - acc: 0.8703 - val_loss: 0.5386 - val_acc: 0.8187\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3679 - acc: 0.8727 - val_loss: 0.5393 - val_acc: 0.8180\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3660 - acc: 0.8728 - val_loss: 0.5402 - val_acc: 0.8190\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3644 - acc: 0.8735 - val_loss: 0.5390 - val_acc: 0.8183\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3625 - acc: 0.8744 - val_loss: 0.5400 - val_acc: 0.8183\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3608 - acc: 0.8754 - val_loss: 0.5406 - val_acc: 0.8180\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3589 - acc: 0.8745 - val_loss: 0.5439 - val_acc: 0.8170\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3577 - acc: 0.8756 - val_loss: 0.5416 - val_acc: 0.8183\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 2s 65us/step - loss: 0.3562 - acc: 0.8759 - val_loss: 0.5418 - val_acc: 0.8183\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3545 - acc: 0.8764 - val_loss: 0.5466 - val_acc: 0.8140\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3526 - acc: 0.8778 - val_loss: 0.5437 - val_acc: 0.8167\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3506 - acc: 0.8790 - val_loss: 0.5469 - val_acc: 0.8150\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3495 - acc: 0.8790 - val_loss: 0.5452 - val_acc: 0.8153\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3480 - acc: 0.8785 - val_loss: 0.5452 - val_acc: 0.8183\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3463 - acc: 0.8798 - val_loss: 0.5464 - val_acc: 0.8177\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3447 - acc: 0.8810 - val_loss: 0.5455 - val_acc: 0.8183\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.3432 - acc: 0.8818 - val_loss: 0.5467 - val_acc: 0.8140\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3418 - acc: 0.8822 - val_loss: 0.5479 - val_acc: 0.8173\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3405 - acc: 0.8817 - val_loss: 0.5469 - val_acc: 0.8190\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3387 - acc: 0.8819 - val_loss: 0.5480 - val_acc: 0.8143\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3377 - acc: 0.8828 - val_loss: 0.5486 - val_acc: 0.8163\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3358 - acc: 0.8840 - val_loss: 0.5506 - val_acc: 0.8173\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3347 - acc: 0.8850 - val_loss: 0.5512 - val_acc: 0.8160\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3333 - acc: 0.8851 - val_loss: 0.5505 - val_acc: 0.8147\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3313 - acc: 0.8863 - val_loss: 0.5503 - val_acc: 0.8180\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3303 - acc: 0.8863 - val_loss: 0.5525 - val_acc: 0.8163\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3284 - acc: 0.8873 - val_loss: 0.5524 - val_acc: 0.8153\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3275 - acc: 0.8884 - val_loss: 0.5530 - val_acc: 0.8150\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3262 - acc: 0.8878 - val_loss: 0.5555 - val_acc: 0.8153\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3246 - acc: 0.8886 - val_loss: 0.5551 - val_acc: 0.8157\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3231 - acc: 0.8886 - val_loss: 0.5562 - val_acc: 0.8160\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3219 - acc: 0.8889 - val_loss: 0.5573 - val_acc: 0.8153\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3206 - acc: 0.8900 - val_loss: 0.5575 - val_acc: 0.8143\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3191 - acc: 0.8907 - val_loss: 0.5584 - val_acc: 0.8127\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3181 - acc: 0.8909 - val_loss: 0.5581 - val_acc: 0.8143\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3165 - acc: 0.8912 - val_loss: 0.5590 - val_acc: 0.8140\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3153 - acc: 0.8914 - val_loss: 0.5606 - val_acc: 0.8140\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3141 - acc: 0.8927 - val_loss: 0.5617 - val_acc: 0.8143\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3130 - acc: 0.8928 - val_loss: 0.5617 - val_acc: 0.8137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3117 - acc: 0.8935 - val_loss: 0.5623 - val_acc: 0.8150\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.3105 - acc: 0.8945 - val_loss: 0.5650 - val_acc: 0.8117\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3089 - acc: 0.8948 - val_loss: 0.5646 - val_acc: 0.8127\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 64us/step\n",
      "4000/4000 [==============================] - 0s 107us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30395204125751146, 0.897090909090909]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5969305039644242, 0.7975]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
